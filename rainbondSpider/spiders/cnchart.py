import scrapy
import re
import yaml
from rainbondSpider.items import PackageItem

class CnchartSpider(scrapy.Spider):
    name = "cnchart"
    allowed_domains = ["example.com"]
    def start_requests(self):
      url_info=[
        'https://datreeio.github.io/admission-webhook-datree/index.yaml',
        'https://charts.apiseven.com/index.yaml',
        'https://datreeio.github.io/admission-webhook-datree/index.yaml',
        'https://vmware-tanzu.github.io/helm-charts/index.yaml',
        'https://kedacore.github.io/charts/index.yaml',
        'https://jaegertracing.github.io/helm-charts/index.yaml',
        'https://apache.github.io/shenyu-helm-chart/index.yaml',
      ]
      for url in url_info:
        yield scrapy.Request(url=url)

    def parse(self, response):
      # 将响应内容解析为字典
      chart_data = yaml.safe_load(response.body)
      # 处理信息
      data = []
      chart_entries = chart_data['entries']
      print('chart_entries',chart_entries)
      for chart_name, chart_versions in chart_entries.items():
        chart_data = {
          "name": chart_name,
          "versions": []
        }
        for entry in chart_versions:
          version_data = {
            "name": entry.get('name', ''),
            "sources": entry.get('sources', []),
            "version": entry.get('version', ''),
            "description": entry.get('description', ''),
            "maintainers": entry.get('maintainers', []),
            "icon": entry.get('icon', ''),
            "apiVersion": entry.get('apiVersion', ''),
            "appVersion": entry.get('appVersion', ''),
            "annotations": entry.get('annotations', {}),
            "urls": entry.get('urls', []),
            "created": entry.get('created', ''),
            "digest": entry.get('digest', ''),
            "dependencies": entry.get('dependencies', [])
          }
          if 'v' not in entry.get('version', ''):
            chart_data['versions'].append(version_data)
        if chart_data['versions']:
          data.append(chart_data)
      # 获取版本号
      for chart in data:
        grouped_versions = {}
        for version in chart["versions"]:
          major, _, _ = self.get_version_parts(version)
          if major not in grouped_versions:
            grouped_versions[major] = []
          grouped_versions[major].append(version['version'])
        merged_array = []
        for values in grouped_versions.values():
          merged_array.extend(values)
        ver_rels = {}
        major_versions = set()
        for version in merged_array:
          if len(version.split('.')) < 3:
            continue
          main_version = version.split('.')[0] + '.' + version.split('.')[1]
          major_versions.add(version.split('.')[0])
          if not ver_rels.get(main_version):
            ver_rels[main_version] = version
          elif ver_rels[main_version] < version:
            ver_rels[main_version] = version
        filtered_list = []
        i = 3
        int_major_versions = [int(major_version) for major_version in major_versions]
        major_versions = sorted(int_major_versions, reverse=True)
        for major_version in major_versions:
          if i == 0:
            break
          versions = []
          for version in merged_array:
            main_version = version.split('.')[0] + '.' + version.split('.')[1]
            if version.startswith(str(major_version) + '.') and (main_version not in versions):
              versions.append(main_version)
          versions.sort(reverse=True,key=lambda x: list(map(int, x.split('.'))))
          filtered_list.extend(versions[:3])
          i -= 1
        results = [ver_rels[version] for version in filtered_list]
        matching_versions = []
        for version in chart["versions"]:
          if version["version"] in results:
            package_item = PackageItem()
            matching_versions.append(version)
            package_item["package_id"] = version.get('digest', "")
            package_item["name"] = version.get('name', "")
            package_item["version"] = version.get('version', "")
            package_item["description"] = version.get('description', "")
            downloadUrl = version.get('urls', "")[0]
            if "github.com" in downloadUrl:
              downloadUrl = "https://ghproxy.com/" + downloadUrl
            package_item["file_urls"] = [downloadUrl]
            package_item["image_urls"] = []
            package_item["logo_image_id"] = ''
            package_item["category"] = 0
            if version.get('icon', ""):
              package_item["image_urls"] = [version.get('icon', "")]
              package_item["logo_image_id"] = version.get('icon', "")
            html_content = self.get_redme(version.get('name', ""))
            # 替换 readme 内容
            if html_content:
              chart_url = 'https://charts.grapps.cn'
              new_repo_url = f"{chart_url}/"
              new_repo_name = f" appstore/{package_item['name']}"
              pattern = r'helm (repo add|install) ([^\s]+) ([^\s]+)'
              result = re.search(pattern, html_content)
              if result:
                repo_name = result.group(2)
                repo_url = result.group(3)
                if repo_url:
                  old_url = repo_url
                  html_content = html_content.replace(old_url, new_repo_url)
                if repo_name:
                  old_name = f" {repo_name} "
                  old_repo_name = f" {repo_name}/{package_item['name']}"
                  html_content = html_content.replace(old_repo_name, new_repo_name)
                  html_content = html_content.replace(old_name, " appstore ")
            package_item["readme"] = html_content
            if downloadUrl.startswith("http://") or downloadUrl.startswith("https://") and version.get('name', "") != 'apisix-ingress-controller':
              yield package_item
    def get_version_parts(self, version):
      v = version
      if type(v) == dict:
        v = version["version"]
      match = re.match(r"(\d+)\.(\d+)\.(\d+)", v)
      if match:
        major = int(match.group(1))
        minor = int(match.group(2))
        patch = int(match.group(3))
        return major, minor, patch
      else:
        return None, None, None
    def get_redme(self, name):
      if name == 'apisix-dashboard':
        return '# Apache APISIX Dashboard\n\n[APISIX Dashboard](https://github.com/apache/apisix-dashboard/) is designed to make it as easy as possible for users to operate Apache APISIX through a frontend interface.\n\nThis chart bootstraps an apisix-dashboard deployment on a [Kubernetes](http://kubernetes.io) cluster using the [Helm](https://helm.sh) package manager.\n\n## Prerequisites\n\nAPISIX Dashboard requires Kubernetes version 1.14+.\n\n## Get Repo Info\n\n```console\nhelm repo add apisix https://charts.apiseven.com\nhelm repo update\n```\n\n## Install Chart\n\n**Important:** only helm3 is supported\n\n```console\nhelm install [RELEASE_NAME] apisix/apisix-dashboard --namespace ingress-apisix --create-namespace\n```\n\nThe command deploys apisix-dashboard on the Kubernetes cluster in the default configuration.\n\n_See [configuration](#configuration) below._\n\n_See [helm install](https://helm.sh/docs/helm/helm_install/) for command documentation._\n\n## Uninstall Chart\n\n```console\nhelm uninstall [RELEASE_NAME] --namespace ingress-apisix\n```\n\nThis removes all the Kubernetes components associated with the chart and deletes the release.\n\n_See [helm uninstall](https://helm.sh/docs/helm/helm_uninstall/) for command documentation._\n\n## Upgrading Chart\n\n```console\nhelm upgrade [RELEASE_NAME] [CHART] --install\n```\n\n_See [helm upgrade](https://helm.sh/docs/helm/helm_upgrade/) for command documentation._\n\n## Parameters\n\n## Values\n\n| Key | Type | Default | Description |\n|-----|------|---------|-------------|\n| affinity | object | `{}` |  |\n| autoscaling.enabled | bool | `false` | Enable autoscaling for Apache APISIX Dashboard deployment |\n| autoscaling.maxReplicas | int | `100` | Maximum number of replicas to scale out |\n| autoscaling.minReplicas | int | `1` | Minimum number of replicas to scale back |\n| autoscaling.targetCPUUtilizationPercentage | int | `80` | Target CPU utilization percentage |\n| autoscaling.version | string | `"v2"` | HPA version, the value is "v2" or "v2beta1", default "v2" |\n| config.authentication.expireTime | int | `3600` | JWT token expire time, in second |\n| config.authentication.secret | string | `"secret"` | Secret for jwt token generation |\n| config.authentication.users | list | `[{"password":"admin","username":"admin"}]` | Specifies username and password for login manager api. |\n| config.conf.etcd.endpoints | list | `["apisix-etcd:2379"]` | Supports defining multiple etcd host addresses for an etcd cluster |\n| config.conf.etcd.password | string | `nil` | Specifies etcd basic auth password if enable etcd auth |\n| config.conf.etcd.prefix | string | `"/apisix"` | apisix configurations prefix |\n| config.conf.etcd.username | string | `nil` | Specifies etcd basic auth username if enable etcd auth |\n| config.conf.listen.host | string | `"0.0.0.0"` | The address on which the Manager API should listen. The default value is 0.0.0.0, if want to specify, please enable it. This value accepts IPv4, IPv6, and hostname. |\n| config.conf.listen.port | int | `9000` | The port on which the Manager API should listen. |\n| config.conf.log.accessLog.filePath | string | `"/dev/stdout"` | Error log path |\n| config.conf.log.errorLog | object | `{"filePath":"/dev/stderr","level":"warn"}` | Error log level. Supports levels, lower to higher: debug, info, warn, error, panic, fatal |\n| config.conf.log.errorLog.filePath | string | `"/dev/stderr"` | Access log path |\n| fullnameOverride | string | `""` | String to fully override apisix-dashboard.fullname template |\n| image.pullPolicy | string | `"IfNotPresent"` | Apache APISIX Dashboard image pull policy |\n| image.repository | string | `"apache/apisix-dashboard"` | Apache APISIX Dashboard image repository |\n| image.tag | string | `"3.0.0-alpine"` |  |\n| imagePullSecrets | list | `[]` | Docker registry secret names as an array |\n| ingress.annotations | object | `{}` | Ingress annotations |\n| ingress.className | string | `""` | Kubernetes 1.18+ support ingressClassName attribute |\n| ingress.enabled | bool | `false` | Set to true to enable ingress record generation |\n| ingress.hosts | list | `[{"host":"apisix-dashboard.local","paths":[]}]` | The list of hostnams to be covered with this ingress record |\n| ingress.tls | list | `[]` | Create TLS Secret |\n| labelsOverride | object | `{}` | Override default labels assigned to Apache APISIX dashboard resource |\n| nameOverride | string | `""` | String to partially override apisix-dashboard.fullname template (will maintain the release name) |\n| nodeSelector | object | `{}` | Node labels for pod assignment |\n| podAnnotations | object | `{}` | Apache APISIX Dashboard Pod annotations |\n| podSecurityContext | object | `{}` | Set the securityContext for Apache APISIX Dashboard pods |\n| priorityClassName | string | `""` | Set the [priorityClassName](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority) for pods |\n| replicaCount | int | `1` | Number of Apache APISIX Dashboard nodes |\n| resources | object | `{}` |  |\n| securityContext | object | `{}` | Set the securityContext for Apache APISIX Dashboard container |\n| service.port | int | `80` | Service HTTP port |\n| service.type | string | `"ClusterIP"` | Service type |\n| serviceAccount.annotations | object | `{}` | Annotations to add to the service account |\n| serviceAccount.create | bool | `true` | Specifies whether a service account should be created |\n| serviceAccount.name | string | `""` | The name of the service account to use. If not set and create is true, a name is generated using the fullname template |\n| tolerations | list | `[]` | Tolerations for pod assignment |\n'
      if name == 'apisix':
        return '## Apache APISIX for Kubernetes\n\nApache APISIX is a dynamic, real-time, high-performance API gateway.\n\nAPISIX provides rich traffic management features such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and more.\n\nYou can use Apache APISIX to handle traditional north-south traffic, as well as east-west traffic between services. It can also be used as a [k8s ingress controller](https://github.com/apache/apisix-ingress-controller/).\n\nThis chart bootstraps all the components needed to run Apache APISIX on a Kubernetes Cluster using [Helm](https://helm.sh).\n\n## Prerequisites\n\n* Kubernetes v1.14+\n* Helm v3+\n\n## Install\n\nTo install the chart with the release name `my-apisix`:\n\n```sh\nhelm repo add apisix https://charts.apiseven.com\nhelm repo update\n\nhelm install [RELEASE_NAME] apisix/apisix --namespace ingress-apisix --create-namespace\n```\n\n## Uninstall\n\n To uninstall/delete a Helm release `my-apisix`:\n\n ```sh\nhelm delete [RELEASE_NAME] --namespace ingress-apisix\n ```\n\nThe command removes all the Kubernetes components associated with the chart and deletes the release.\n\n## Parameters\n\n## Values\n\n| Key | Type | Default | Description |\n|-----|------|---------|-------------|\n| affinity | object | `{}` | Set affinity for Apache APISIX deploy |\n| apisix.admin.allow.ipList | list | `["127.0.0.1/24"]` | The client IP CIDR allowed to access Apache APISIX Admin API service. |\n| apisix.admin.cors | bool | `true` | Admin API support CORS response headers |\n| apisix.admin.credentials | object | `{"admin":"edd1c9f034335f136f87ad84b625c8f1","secretName":"","viewer":"4054f7cf07e344346cd3f287985e76a2"}` | Admin API credentials |\n| apisix.admin.credentials.admin | string | `"edd1c9f034335f136f87ad84b625c8f1"` | Apache APISIX admin API admin role credentials |\n| apisix.admin.credentials.secretName | string | `""` | The APISIX Helm chart supports storing user credentials in a secret. The secret needs to contain two keys, admin and viewer, with their respective values set. |\n| apisix.admin.credentials.viewer | string | `"4054f7cf07e344346cd3f287985e76a2"` | Apache APISIX admin API viewer role credentials |\n| apisix.admin.enabled | bool | `true` | Enable Admin API |\n| apisix.admin.externalIPs | list | `[]` | IPs for which nodes in the cluster will also accept traffic for the servic |\n| apisix.admin.ingress | object | `{"annotations":{},"enabled":false,"hosts":[{"host":"apisix-admin.local","paths":["/apisix"]}],"tls":[]}` | Using ingress access Apache APISIX admin service |\n| apisix.admin.ingress.annotations | object | `{}` | Ingress annotations |\n| apisix.admin.ip | string | `"0.0.0.0"` | which ip to listen on for Apache APISIX admin API. Set to `"[::]"` when on IPv6 single stack |\n| apisix.admin.port | int | `9180` | which port to use for Apache APISIX admin API |\n| apisix.admin.servicePort | int | `9180` | Service port to use for Apache APISIX admin API |\n| apisix.admin.type | string | `"ClusterIP"` | admin service type |\n| apisix.customPlugins | object | `{"enabled":false,"luaPath":"/opts/custom_plugins/?.lua","plugins":[{"attrs":{},"configMap":{"mounts":[{"key":"the-file-name","path":"mount-path"}],"name":"configmap-name"},"name":"plugin-name"}]}` | customPlugins allows you to mount your own HTTP plugins. |\n| apisix.customPlugins.enabled | bool | `false` | Whether to configure some custom plugins |\n| apisix.customPlugins.luaPath | string | `"/opts/custom_plugins/?.lua"` | the lua_path that tells APISIX where it can find plugins, note the last \';\' is required. |\n| apisix.customPlugins.plugins[0] | object | `{"attrs":{},"configMap":{"mounts":[{"key":"the-file-name","path":"mount-path"}],"name":"configmap-name"},"name":"plugin-name"}` | plugin name. |\n| apisix.customPlugins.plugins[0].attrs | object | `{}` | plugin attrs |\n| apisix.customPlugins.plugins[0].configMap | object | `{"mounts":[{"key":"the-file-name","path":"mount-path"}],"name":"configmap-name"}` | plugin codes can be saved inside configmap object. |\n| apisix.customPlugins.plugins[0].configMap.mounts | list | `[{"key":"the-file-name","path":"mount-path"}]` | since keys in configmap is flat, mountPath allows to define the mount path, so that plugin codes can be mounted hierarchically. |\n| apisix.customPlugins.plugins[0].configMap.name | string | `"configmap-name"` | name of configmap. |\n| apisix.deployment.certs | object | `{"cert":"","cert_key":"","certsSecret":"","mTLSCACert":"","mTLSCACertSecret":""}` | certs used for certificates in decoupled mode |\n| apisix.deployment.certs.cert | string | `""` | cert name in certsSecret |\n| apisix.deployment.certs.cert_key | string | `""` | cert key in certsSecret |\n| apisix.deployment.certs.certsSecret | string | `""` | secret name used for decoupled mode |\n| apisix.deployment.certs.mTLSCACert | string | `""` | mTLS CA cert filename in mTLSCACertSecret |\n| apisix.deployment.certs.mTLSCACertSecret | string | `""` | trusted_ca_cert name in certsSecret |\n| apisix.deployment.controlPlane | object | `{"cert":"","certKey":"","certsSecret":"","confServerPort":"9280"}` | used for control_plane deployment mode |\n| apisix.deployment.controlPlane.cert | string | `""` | conf Server CA cert name in certsSecret |\n| apisix.deployment.controlPlane.certKey | string | `""` | conf Server cert key name in certsSecret |\n| apisix.deployment.controlPlane.certsSecret | string | `""` | secret name used by conf Server |\n| apisix.deployment.controlPlane.confServerPort | string | `"9280"` | conf Server address |\n| apisix.deployment.dataPlane | object | `{"controlPlane":{"host":[],"prefix":"/apisix","timeout":30}}` | used for data_plane deployment mode |\n| apisix.deployment.dataPlane.controlPlane.host | list | `[]` | The hosts of the control_plane used by the data_plane |\n| apisix.deployment.dataPlane.controlPlane.prefix | string | `"/apisix"` | The prefix of the control_plane used by the data_plane |\n| apisix.deployment.dataPlane.controlPlane.timeout | int | `30` | Timeout when the data plane connects to the control plane |\n| apisix.deployment.mode | string | `"traditional"` | Apache APISIX deployment mode Optional: traditional, decoupled  ref: https://apisix.apache.org/docs/apisix/deployment-modes/ |\n| apisix.deployment.role | string | `"traditional"` | Deployment role Optional: traditional, data_plane, control_plane  ref: https://apisix.apache.org/docs/apisix/deployment-modes/ |\n| apisix.discovery.enabled | bool | `false` | Enable or disable Apache APISIX integration service discovery |\n| apisix.discovery.registry | object | `{}` | Registry is the same to the one in APISIX [config-default.yaml](https://github.com/apache/apisix/blob/master/conf/config-default.yaml#L281), and refer to such file for more setting details. also refer to [this documentation for integration service discovery](https://apisix.apache.org/docs/apisix/discovery) |\n| apisix.dns.resolvers[0] | string | `"127.0.0.1"` |  |\n| apisix.dns.resolvers[1] | string | `"172.20.0.10"` |  |\n| apisix.dns.resolvers[2] | string | `"114.114.114.114"` |  |\n| apisix.dns.resolvers[3] | string | `"223.5.5.5"` |  |\n| apisix.dns.resolvers[4] | string | `"1.1.1.1"` |  |\n| apisix.dns.resolvers[5] | string | `"8.8.8.8"` |  |\n| apisix.dns.timeout | int | `5` |  |\n| apisix.dns.validity | int | `30` |  |\n| apisix.enableIPv6 | bool | `true` | Enable nginx IPv6 resolver |\n| apisix.enableServerTokens | bool | `true` | Whether the APISIX version number should be shown in Server header |\n| apisix.extPlugin.cmd | list | `["/path/to/apisix-plugin-runner/runner","run"]` | the command and its arguements to run as a subprocess |\n| apisix.extPlugin.enabled | bool | `false` | Enable External Plugins. See [external plugin](https://apisix.apache.org/docs/apisix/next/external-plugin/) |\n| apisix.fullCustomConfig.config | object | `{}` | If apisix.fullCustomConfig.enabled is true, full customized config.yaml. Please note that other settings about APISIX config will be ignored |\n| apisix.fullCustomConfig.enabled | bool | `false` | Enable full customized config.yaml |\n| apisix.luaModuleHook | object | `{"configMapRef":{"mounts":[{"key":"","path":""}],"name":""},"enabled":false,"hookPoint":"","luaPath":""}` | Whether to add a custom lua module |\n| apisix.luaModuleHook.configMapRef | object | `{"mounts":[{"key":"","path":""}],"name":""}` | configmap that stores the codes |\n| apisix.luaModuleHook.configMapRef.mounts[0] | object | `{"key":"","path":""}` | Name of the ConfigMap key, for setting the mapping relationship between ConfigMap key and the lua module code path. |\n| apisix.luaModuleHook.configMapRef.mounts[0].path | string | `""` | Filepath of the plugin code, for setting the mapping relationship between ConfigMap key and the lua module code path. |\n| apisix.luaModuleHook.configMapRef.name | string | `""` | Name of the ConfigMap where the lua module codes store |\n| apisix.luaModuleHook.hookPoint | string | `""` | the hook module which will be used to inject third party code into APISIX use the lua require style like: "module.say_hello" |\n| apisix.luaModuleHook.luaPath | string | `""` | extend lua_package_path to load third party code |\n| apisix.nginx.configurationSnippet | object | `{"httpAdmin":"","httpEnd":"","httpSrv":"","httpStart":"","main":"","stream":""}` | Custom configuration snippet. |\n| apisix.nginx.customLuaSharedDicts | list | `[]` | Add custom [lua_shared_dict](https://github.com/openresty/lua-nginx-module#toc88) settings, click [here](https://github.com/apache/apisix-helm-chart/blob/master/charts/apisix/values.yaml#L27-L30) to learn the format of a shared dict |\n| apisix.nginx.enableCPUAffinity | bool | `true` |  |\n| apisix.nginx.envs | list | `[]` |  |\n| apisix.nginx.logs.accessLog | string | `"/dev/stdout"` | Access log path |\n| apisix.nginx.logs.accessLogFormat | string | `"$remote_addr - $remote_user [$time_local] $http_host \\\\\\"$request\\\\\\" $status $body_bytes_sent $request_time \\\\\\"$http_referer\\\\\\" \\\\\\"$http_user_agent\\\\\\" $upstream_addr $upstream_status $upstream_response_time \\\\\\"$upstream_scheme://$upstream_host$upstream_uri\\\\\\""` | Access log format |\n| apisix.nginx.logs.accessLogFormatEscape | string | `"default"` | Allows setting json or default characters escaping in variables |\n| apisix.nginx.logs.enableAccessLog | bool | `true` | Enable access log or not, default true |\n| apisix.nginx.logs.errorLog | string | `"/dev/stderr"` | Error log path |\n| apisix.nginx.logs.errorLogLevel | string | `"warn"` | Error log level |\n| apisix.nginx.workerConnections | string | `"10620"` |  |\n| apisix.nginx.workerProcesses | string | `"auto"` |  |\n| apisix.nginx.workerRlimitNofile | string | `"20480"` |  |\n| apisix.pluginAttrs | object | `{}` | Set APISIX plugin attributes, see [config-default.yaml](https://github.com/apache/apisix/blob/master/conf/config-default.yaml#L376) for more details |\n| apisix.plugins | list | `[]` | Customize the list of APISIX plugins to enable. By default, APISIX\'s default plugins are automatically used. See [config-default.yaml](https://github.com/apache/apisix/blob/master/conf/config-default.yaml) |\n| apisix.prometheus.containerPort | int | `9091` | container port where the metrics are exposed |\n| apisix.prometheus.enabled | bool | `false` |  |\n| apisix.prometheus.metricPrefix | string | `"apisix_"` | prefix of the metrics |\n| apisix.prometheus.path | string | `"/apisix/prometheus/metrics"` | path of the metrics endpoint |\n| apisix.router.http | string | `"radixtree_host_uri"` | Defines how apisix handles routing: - radixtree_uri: match route by uri(base on radixtree) - radixtree_host_uri: match route by host + uri(base on radixtree) - radixtree_uri_with_parameter: match route by uri with parameters |\n| apisix.setIDFromPodUID | bool | `false` | Use Pod metadata.uid as the APISIX id. |\n| apisix.ssl.additionalContainerPorts | list | `[]` | Support multiple https ports, See [Configuration](https://github.com/apache/apisix/blob/0bc65ea9acd726f79f80ae0abd8f50b7eb172e3d/conf/config-default.yaml#L99) |\n| apisix.ssl.certCAFilename | string | `""` | Filename be used in the apisix.ssl.existingCASecret |\n| apisix.ssl.containerPort | int | `9443` |  |\n| apisix.ssl.enabled | bool | `false` |  |\n| apisix.ssl.existingCASecret | string | `""` | Specifies the name of Secret contains trusted CA certificates in the PEM format used to verify the certificate when APISIX needs to do SSL/TLS handshaking with external services (e.g. etcd) |\n| apisix.ssl.http2.enabled | bool | `true` |  |\n| apisix.ssl.sslProtocols | string | `"TLSv1.2 TLSv1.3"` | TLS protocols allowed to use. |\n| apisix.stream_plugins | list | `[]` | Customize the list of APISIX stream_plugins to enable. By default, APISIX\'s default stream_plugins are automatically used. See [config-default.yaml](https://github.com/apache/apisix/blob/master/conf/config-default.yaml) |\n| apisix.vault.enabled | bool | `false` | Enable or disable the vault integration |\n| apisix.vault.host | string | `""` | The host address where the vault server is running. |\n| apisix.vault.prefix | string | `""` | Prefix allows you to better enforcement of policies. |\n| apisix.vault.timeout | int | `10` | HTTP timeout for each request. |\n| apisix.vault.token | string | `""` | The generated token from vault instance that can grant access to read data from the vault. |\n| apisix.wasm.enabled | bool | `false` | Enable Wasm Plugins. See [wasm plugin](https://apisix.apache.org/docs/apisix/next/wasm/) |\n| apisix.wasm.plugins | list | `[]` |  |\n| autoscaling.enabled | bool | `false` |  |\n| autoscaling.maxReplicas | int | `100` |  |\n| autoscaling.minReplicas | int | `1` |  |\n| autoscaling.targetCPUUtilizationPercentage | int | `80` |  |\n| autoscaling.targetMemoryUtilizationPercentage | int | `80` |  |\n| autoscaling.version | string | `"v2"` | HPA version, the value is "v2" or "v2beta1", default "v2" |\n| dashboard.config.conf.etcd.endpoints | list | `["apisix-etcd:2379"]` | Supports defining multiple etcd host addresses for an etcd cluster |\n| dashboard.config.conf.etcd.password | string | `nil` | Specifies etcd basic auth password if enable etcd auth |\n| dashboard.config.conf.etcd.prefix | string | `"/apisix"` | apisix configurations prefix |\n| dashboard.config.conf.etcd.username | string | `nil` | Specifies etcd basic auth username if enable etcd auth |\n| dashboard.enabled | bool | `false` |  |\n| etcd | object | `{"auth":{"rbac":{"create":false,"rootPassword":""},"tls":{"certFilename":"","certKeyFilename":"","enabled":false,"existingSecret":"","sni":"","verify":true}},"enabled":true,"prefix":"/apisix","replicaCount":3,"service":{"port":2379},"timeout":30}` | etcd configuration use the FQDN address or the IP of the etcd |\n| etcd.auth | object | `{"rbac":{"create":false,"rootPassword":""},"tls":{"certFilename":"","certKeyFilename":"","enabled":false,"existingSecret":"","sni":"","verify":true}}` | if etcd.enabled is true, set more values of bitnami/etcd helm chart |\n| etcd.auth.rbac.create | bool | `false` | No authentication by default. Switch to enable RBAC authentication |\n| etcd.auth.rbac.rootPassword | string | `""` | root password for etcd. Requires etcd.auth.rbac.create to be true. |\n| etcd.auth.tls.certFilename | string | `""` | etcd client cert filename using in etcd.auth.tls.existingSecret |\n| etcd.auth.tls.certKeyFilename | string | `""` | etcd client cert key filename using in etcd.auth.tls.existingSecret |\n| etcd.auth.tls.enabled | bool | `false` | enable etcd client certificate |\n| etcd.auth.tls.existingSecret | string | `""` | name of the secret contains etcd client cert |\n| etcd.auth.tls.sni | string | `""` | specify the TLS Server Name Indication extension, the ETCD endpoint hostname will be used when this setting is unset. |\n| etcd.auth.tls.verify | bool | `true` | whether to verify the etcd endpoint certificate when setup a TLS connection to etcd |\n| etcd.enabled | bool | `true` | install etcd(v3) by default, set false if do not want to install etcd(v3) together |\n| etcd.prefix | string | `"/apisix"` | apisix configurations prefix |\n| etcd.timeout | int | `30` | Set the timeout value in seconds for subsequent socket operations from apisix to etcd cluster |\n| externalEtcd | object | `{"existingSecret":"","host":["http://etcd.host:2379"],"password":"","secretPasswordKey":"etcd-root-password","user":"root"}` | external etcd configuration. If etcd.enabled is false, these configuration will be used. |\n| externalEtcd.existingSecret | string | `""` | if externalEtcd.existingSecret is the name of secret containing the external etcd password |\n| externalEtcd.host | list | `["http://etcd.host:2379"]` | if etcd.enabled is false, use external etcd, support multiple address, if your etcd cluster enables TLS, please use https scheme, e.g. https://127.0.0.1:2379. |\n| externalEtcd.password | string | `""` | if etcd.enabled is false and externalEtcd.existingSecret is empty, externalEtcd.password is the passsword for external etcd. |\n| externalEtcd.secretPasswordKey | string | `"etcd-root-password"` | externalEtcd.secretPasswordKey Key inside the secret containing the external etcd password |\n| externalEtcd.user | string | `"root"` | if etcd.enabled is false, user for external etcd. Set empty to disable authentication |\n| extraEnvVars | list | `[]` | extraEnvVars An array to add extra env vars e.g: extraEnvVars:   - name: FOO     value: "bar"   - name: FOO2     valueFrom:       secretKeyRef:         name: SECRET_NAME         key: KEY |\n| extraInitContainers | list | `[]` | Additional `initContainers`, See [Kubernetes initContainers](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) for the detail. |\n| extraVolumeMounts | list | `[]` | Additional `volume`, See [Kubernetes Volumes](https://kubernetes.io/docs/concepts/storage/volumes/) for the detail. |\n| extraVolumes | list | `[]` | Additional `volume`, See [Kubernetes Volumes](https://kubernetes.io/docs/concepts/storage/volumes/) for the detail. |\n| fullnameOverride | string | `""` |  |\n| global.imagePullSecrets | list | `[]` | Global Docker registry secret names as an array |\n| hostNetwork | bool | `false` |  |\n| image.pullPolicy | string | `"IfNotPresent"` | Apache APISIX image pull policy |\n| image.repository | string | `"apache/apisix"` | Apache APISIX image repository |\n| image.tag | string | `"3.4.0-debian"` | Apache APISIX image tag Overrides the image tag whose default is the chart appVersion. |\n| ingress | object | `{"annotations":{},"enabled":false,"hosts":[{"host":"apisix.local","paths":[]}],"tls":[]}` | Using ingress access Apache APISIX service |\n| ingress-controller | object | `{"config":{"apisix":{"adminAPIVersion":"v3"}},"enabled":false}` | Ingress controller configuration |\n| ingress.annotations | object | `{}` | Ingress annotations |\n| initContainer.image | string | `"busybox"` | Init container image |\n| initContainer.tag | float | `1.28` | Init container tag |\n| metrics | object | `{"serviceMonitor":{"annotations":{},"enabled":false,"interval":"15s","labels":{},"name":"","namespace":""}}` | Observability configuration. |\n| metrics.serviceMonitor.annotations | object | `{}` | @param serviceMonitor.annotations ServiceMonitor annotations |\n| metrics.serviceMonitor.enabled | bool | `false` | Enable or disable Apache APISIX serviceMonitor |\n| metrics.serviceMonitor.interval | string | `"15s"` | interval at which metrics should be scraped |\n| metrics.serviceMonitor.labels | object | `{}` | @param serviceMonitor.labels ServiceMonitor extra labels |\n| metrics.serviceMonitor.name | string | `""` | name of the serviceMonitor, by default, it is the same as the apisix fullname |\n| metrics.serviceMonitor.namespace | string | `""` | namespace where the serviceMonitor is deployed, by default, it is the same as the namespace of the apisix |\n| nameOverride | string | `""` |  |\n| nodeSelector | object | `{}` | Node labels for Apache APISIX pod assignment |\n| podAnnotations | object | `{}` | Annotations to add to each pod |\n| podDisruptionBudget | object | `{"enabled":false,"maxUnavailable":1,"minAvailable":"90%"}` | See https://kubernetes.io/docs/tasks/run-application/configure-pdb/ for more details |\n| podDisruptionBudget.enabled | bool | `false` | Enable or disable podDisruptionBudget |\n| podDisruptionBudget.maxUnavailable | int | `1` | Set the maxUnavailable of podDisruptionBudget |\n| podDisruptionBudget.minAvailable | string | `"90%"` | Set the `minAvailable` of podDisruptionBudget. You can specify only one of `maxUnavailable` and `minAvailable` in a single PodDisruptionBudget. See [Specifying a Disruption Budget for your Application](https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget) for more details |\n| podSecurityContext | object | `{}` | Set the securityContext for Apache APISIX pods |\n| priorityClassName | string | `""` | Set [priorityClassName](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority) for Apache APISIX pods |\n| rbac.create | bool | `false` |  |\n| replicaCount | int | `1` | if useDaemonSet is true or autoscaling.enabled is true, replicaCount not become effective |\n| resources | object | `{}` | Set pod resource requests & limits |\n| securityContext | object | `{}` | Set the securityContext for Apache APISIX container |\n| service.externalIPs | list | `[]` |  |\n| service.externalTrafficPolicy | string | `"Cluster"` |  |\n| service.http | object | `{"additionalContainerPorts":[],"containerPort":9080,"enabled":true,"servicePort":80}` | Apache APISIX service settings for http |\n| service.http.additionalContainerPorts | list | `[]` | Support multiple http ports, See [Configuration](https://github.com/apache/apisix/blob/0bc65ea9acd726f79f80ae0abd8f50b7eb172e3d/conf/config-default.yaml#L24) |\n| service.labelsOverride | object | `{}` | Override default labels assigned to Apache APISIX gateway resources |\n| service.stream | object | `{"enabled":false,"only":false,"tcp":[],"udp":[]}` | Apache APISIX service settings for stream. L4 proxy (TCP/UDP) |\n| service.tls | object | `{"servicePort":443}` | Apache APISIX service settings for tls |\n| service.type | string | `"NodePort"` | Apache APISIX service type for user access itself |\n| serviceAccount.annotations | object | `{}` |  |\n| serviceAccount.create | bool | `false` |  |\n| serviceAccount.name | string | `""` |  |\n| timezone | string | `""` | timezone is the timezone where apisix uses. For example: "UTC" or "Asia/Shanghai" This value will be set on apisix container\'s environment variable TZ. You may need to set the timezone to be consistent with your local time zone, otherwise the apisix\'s logs may used to retrieve event maybe in wrong timezone. |\n| tolerations | list | `[]` | List of node taints to tolerate |\n| updateStrategy | object | `{}` |  |\n| useDaemonSet | bool | `false` | set false to use `Deployment`, set true to use `DaemonSet` |\n'
      if name == 'jaeger':
        return '# Jaeger\n\n[Jaeger](https://www.jaegertracing.io/) is a distributed tracing system.\n\n## Introduction\n\nThis chart adds all components required to run Jaeger as described in the [jaeger-kubernetes](https://github.com/jaegertracing/jaeger-kubernetes) GitHub page for a production-like deployment. The chart default will deploy a new Cassandra cluster (using the [cassandra chart](https://github.com/kubernetes/charts/tree/master/incubator/cassandra)), but also supports using an existing Cassandra cluster, deploying a new ElasticSearch cluster (using the [elasticsearch chart](https://github.com/elastic/helm-charts/tree/master/elasticsearch)), or connecting to an existing ElasticSearch cluster. Once the storage backend is available, the chart will deploy jaeger-agent as a DaemonSet and deploy the jaeger-collector and jaeger-query components as Deployments.\n\n## Installing the Chart\n\nAdd the Jaeger Tracing Helm repository:\n\n```bash\nhelm repo add jaegertracing https://jaegertracing.github.io/helm-charts\n```\n\nTo install a release named `jaeger`:\n\n```bash\nhelm install jaeger jaegertracing/jaeger\n```\n\nBy default, the chart deploys the following:\n\n- Jaeger Agent DaemonSet\n- Jaeger Collector Deployment\n- Jaeger Query (UI) Deployment\n- Cassandra StatefulSet (subject to change!)\n\n![Jaeger with Default\ncomponents](https://www.jaegertracing.io/img/architecture-v1.png)\n\n## Configuration\n\nSee [Customizing the Chart Before Installing](https://helm.sh/docs/intro/using_helm/#customizing-the-chart-before-installing). To see all configurable options with detailed comments, visit the chart\'s [values.yaml](https://github.com/jaegertracing/helm-charts/blob/master/charts/jaeger/values.yaml), or run these configuration commands:\n\n```console\n$ helm show values jaegertracing/jaeger\n```\n\nYou may also `helm show values` on this chart\'s [dependencies](#dependencies) for additional options.\n\n### Dependencies\n\nIf installing with a dependency such as Cassandra, Elasticsearch and/or Kafka\ntheir, values can be shown by running:\n\n```console\nhelm repo add elastic https://helm.elastic.co\nhelm show values elastic/elasticsearch\n```\n\n```console\nhelm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com/\nhelm show values incubator/cassandra\n```\n\n```console\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm show values bitnami/kafka\n```\n\nPlease note, any dependency values must be nested within the key named after the\nchart, i.e. `elasticsearch`, `cassandra` and/or `kafka`.\n\n## Storage\n\nAs per Jaeger documentation, for large scale production deployment the Jaeger\nteam [recommends Elasticsearch backend over Cassandra](https://www.jaegertracing.io/docs/latest/faq/#what-is-the-recommended-storage-backend),\nas such the default backend may change in the future and **it is highly\nrecommended to explicitly configure storage**.\n\nIf you are just starting out with a testing/demo setup, you can also use in-memory storage for a\nfast and easy setup experience using the [Jaeger All in One executable](https://www.jaegertracing.io/docs/1.29/getting-started/#all-in-one).\n\n### Elasticsearch configuration\n\n#### Elasticsearch Rollover\n\nIf using the [Elasticsearch\nRollover](https://www.jaegertracing.io/docs/latest/deployment/#elasticsearch-rollover)\nfeature, elasticsearch must already be present and so must be deployed\nseparately from this chart, if not the rollover init hook won\'t be able to\ncomplete successfully.\n\n#### Installing the Chart using a New ElasticSearch Cluster\n\nTo install the chart with the release name `jaeger` using a new ElasticSearch cluster instead of Cassandra (default), run the following command:\n\n```console\nhelm install jaeger jaegertracing/jaeger \\\n  --set provisionDataStore.cassandra=false \\\n  --set provisionDataStore.elasticsearch=true \\\n  --set storage.type=elasticsearch\n```\n\n#### Installing the Chart using an Existing Elasticsearch Cluster\n\nA release can be configured as follows to use an existing ElasticSearch cluster as it as the storage backend:\n\n```console\nhelm install jaeger jaegertracing/jaeger \\\n  --set provisionDataStore.cassandra=false \\\n  --set storage.type=elasticsearch \\\n  --set storage.elasticsearch.host=<HOST> \\\n  --set storage.elasticsearch.port=<PORT> \\\n  --set storage.elasticsearch.user=<USER> \\\n  --set storage.elasticsearch.password=<password>\n```\n\n#### Installing the Chart using an Existing ElasticSearch Cluster with TLS\n\nIf you already have an existing running ElasticSearch cluster with TLS, you can configure the chart as follows to use it as your backing store:\n\nContent of the `jaeger-values.yaml` file:\n\n```YAML\nstorage:\n  type: elasticsearch\n  elasticsearch:\n    host: <HOST>\n    port: <PORT>\n    scheme: https\n    user: <USER>\n    password: <PASSWORD>\nprovisionDataStore:\n  cassandra: false\n  elasticsearch: false\nquery:\n  cmdlineParams:\n    es.tls.ca: "/tls/es.pem"\n  extraConfigmapMounts:\n    - name: jaeger-tls\n      mountPath: /tls\n      subPath: ""\n      configMap: jaeger-tls\n      readOnly: true\ncollector:\n  cmdlineParams:\n    es.tls.ca: "/tls/es.pem"\n  extraConfigmapMounts:\n    - name: jaeger-tls\n      mountPath: /tls\n      subPath: ""\n      configMap: jaeger-tls\n      readOnly: true\nspark:\n  enabled: true\n  cmdlineParams:\n    java.opts: "-Djavax.net.ssl.trustStore=/tls/trust.store -Djavax.net.ssl.trustStorePassword=changeit"\n  extraConfigmapMounts:\n    - name: jaeger-tls\n      mountPath: /tls\n      subPath: ""\n      configMap: jaeger-tls\n      readOnly: true\n\n```\n\nGenerate configmap jaeger-tls:\n\n```console\nkeytool -import -trustcacerts -keystore trust.store -storepass changeit -alias es-root -file es.pem\nkubectl create configmap jaeger-tls --from-file=trust.store --from-file=es.pem\n```\n\n```console\nhelm install jaeger jaegertracing/jaeger --values jaeger-values.yaml\n```\n\n### Cassandra configuration\n\n#### Installing the Chart using an Existing Cassandra Cluster\n\nIf you already have an existing running Cassandra cluster, you can configure the chart as follows to use it as your backing store (make sure you replace `<HOST>`, `<PORT>`, etc with your values):\n\n```console\nhelm install jaeger jaegertracing/jaeger \\\n  --set provisionDataStore.cassandra=false \\\n  --set storage.cassandra.host=<HOST> \\\n  --set storage.cassandra.port=<PORT> \\\n  --set storage.cassandra.user=<USER> \\\n  --set storage.cassandra.password=<PASSWORD>\n```\n\n#### Installing the Chart using an Existing Cassandra Cluster with TLS\n\nIf you already have an existing running Cassandra cluster with TLS, you can configure the chart as follows to use it as your backing store:\n\nContent of the `values.yaml` file:\n\n```YAML\nstorage:\n  type: cassandra\n  cassandra:\n    host: <HOST>\n    port: <PORT>\n    user: <USER>\n    password: <PASSWORD>\n    tls:\n      enabled: true\n      secretName: cassandra-tls-secret\n\nprovisionDataStore:\n  cassandra: false\n```\n\nContent of the `jaeger-tls-cassandra-secret.yaml` file:\n\n```YAML\napiVersion: v1\nkind: Secret\nmetadata:\n  name: cassandra-tls-secret\ndata:\n  commonName: <SERVER NAME>\n  ca-cert.pem: |\n    -----BEGIN CERTIFICATE-----\n    <CERT>\n    -----END CERTIFICATE-----\n  client-cert.pem: |\n    -----BEGIN CERTIFICATE-----\n    <CERT>\n    -----END CERTIFICATE-----\n  client-key.pem: |\n    -----BEGIN RSA PRIVATE KEY-----\n    -----END RSA PRIVATE KEY-----\n  cqlshrc: |\n    [ssl]\n    certfile = ~/.cassandra/ca-cert.pem\n    userkey = ~/.cassandra/client-key.pem\n    usercert = ~/.cassandra/client-cert.pem\n\n```\n\n```console\nkubectl apply -f jaeger-tls-cassandra-secret.yaml\nhelm install jaeger jaegertracing/jaeger --values values.yaml\n```\n\n### Ingester Configuration\n\n#### Installing the Chart with Ingester enabled\n\nThe architecture illustrated below can be achieved by enabling the ingester component. When enabled, Cassandra or Elasticsearch (depending on the configured values) now becomes the ingester\'s storage backend, whereas Kafka becomes the storage backend of the collector service.\n\n![Jaeger with Ingester](https://www.jaegertracing.io/img/architecture-v2.png)\n\n#### Installing the Chart with Ingester enabled using a New Kafka Cluster\n\nTo provision a new Kafka cluster along with jaeger-ingester:\n\n```console\nhelm install jaeger jaegertracing/jaeger \\\n  --set provisionDataStore.kafka=true \\\n  --set ingester.enabled=true\n```\n\n#### Installing the Chart with Ingester using an existing Kafka Cluster\n\nYou can use an existing Kafka cluster with jaeger too\n\n```console\nhelm install jaeger jaegertracing/jaeger \\\n  --set ingester.enabled=true \\\n  --set storage.kafka.brokers={<BROKER1:PORT>,<BROKER2:PORT>} \\\n  --set storage.kafka.topic=<TOPIC>\n```\n\n### Other Storage Configuration\n\nIf you are using grpc-plugin based storage, you can set environment\nvariables that are needed by the plugin.\n\nAs as example if using the [jaeger-mongodb](https://github.com/mongodb-labs/jaeger-mongodb)\nplugin you can set the `MONGO_URL` as follows...\n\n```YAML\nstorage:\n  type: grpc-plugin\n  grpcPlugin:\n    extraEnv:\n      - name: MONGO_URL\n        valueFrom:\n          secretKeyRef:\n            key: MONGO_URL\n            name: jaeger-secrets\n```\n\n### All in One In-Memory Configuration\n\n#### Installing the Chart using the All in One executable and in-memory storage\n\nTo install the chart with the release name `jaeger` using in-memory storage and the All in One\nexecutable, configure the chart as follows:\n\nContent of the `values.yaml` file:\n\n```yaml\nprovisionDataStore:\n  cassandra: false\nallInOne:\n  enabled: true\nstorage:\n  type: none\nagent:\n  enabled: false\ncollector:\n  enabled: false\nquery:\n  enabled: false\n```\n\nIt\'s possible to specify resources, extra environment variables, and extra secrets for the all in one deployment:\n\n```yaml\nallInOne:\n  extraEnv:\n    - name: QUERY_BASE_PATH\n      value: /jaeger\n  resources:\n    limits:\n      cpu: 500m\n      memory: 512Mi\n    requests:\n      cpu: 256m\n      memory: 128Mi\n  extraSecretMounts:\n    - name: jaeger-tls\n      mountPath: /tls\n      subPath: ""\n      secretName: jaeger-tls\n      readOnly: true\n```\n\n```bash\nhelm install jaeger jaegertracing/jaeger --values values.yaml\n```\n\n## oAuth2 Sidecar\nIf extra protection of the Jaeger UI is needed, then the oAuth2 sidecar can be enabled in the Jaeger Query. The oAuth2\nsidecar acts as a security proxy in front of the Jaeger Query service and enforces user authentication before reaching\nthe Jaeger UI. This method can work with any valid provider including Keycloak, Azure, Google, GitHub, and more.\n\nOffical docs [here](https://oauth2-proxy.github.io/oauth2-proxy/docs/behaviour)\n\nContent of the `jaeger-values.yaml` file:\n\n```YAML\nquery:\n  enabled: true\n  oAuthSidecar:\n    enabled: true\n    image: quay.io/oauth2-proxy/oauth2-proxy:v7.3.0\n    pullPolicy: IfNotPresent\n    containerPort: 4180\n    args:\n      - --config\n      - /etc/oauth2-proxy/oauth2-proxy.cfg\n      - --client-secret\n      - "$(client-secret)"\n    extraEnv:\n      - name: client-secret\n        valueFrom:\n          secretKeyRef:\n            name: client-secret\n            key: client-secret-key\n    extraConfigmapMounts: []\n    extraSecretMounts: []\n    config: |-\n      provider = "oidc"\n      https_address = ":4180"\n      upstreams = ["http://localhost:16686"]\n      redirect_url = "https://jaeger-svc-domain/oauth2/callback"\n      client_id = "jaeger-query"\n      oidc_issuer_url = "https://keycloak-svc-domain/auth/realms/Default"\n      cookie_secure = "true"\n      cookie_secret = ""\n      email_domains = "*"\n      oidc_groups_claim = "groups"\n      user_id_claim = "preferred_username"\n      skip_provider_button = "true"\n```\n\n## Installing extra kubernetes objects\n\nIf additional kubernetes objects need to be installed alongside this chart, set the `extraObjects` array to contain\nthe yaml describing these objects. The values in the array are treated as a template to allow the use of variable\nsubstitution and function calls as in the example below.\n\nContent of the `jaeger-values.yaml` file:\n\n```YAML\nextraObjects:\n  - apiVersion: rbac.authorization.k8s.io/v1\n    kind: RoleBinding\n    metadata:\n      name: {{ .Release.Name }}-someRoleBinding\n    roleRef:\n      apiGroup: rbac.authorization.k8s.io\n      kind: Role\n      name: someRole\n    subjects:\n      - kind: ServiceAccount\n        name: "{{ include \\"jaeger.esLookback.serviceAccountName\\" . }}"\n```\n\n## Configuring the hotrod example application to send traces to the OpenTelemetry collector\n\nIf the `hotrod` example application is enabled it will export traces to Jaeger\nvia the Jaeger exporter. To switch this to another collector and/or protocol,\nsuch as an OpenTelemetry OTLP Collector, see the example below.\n\nThe primary use case of sending the traces to the collector instead of directly\nto Jaeger is to verify traces can get back to Jaeger or another distributed\ntracing store and verify that pipeline with the pre-instrumented hotrod\napplication.\n\n**NOTE: This will not install or setup the OpenTelemetry collector. To setup an example OpenTelemetry Collector, see the [OpenTelemetry helm\ncharts](https://github.com/open-telemetry/opentelemetry-helm-charts/tree/main/charts/opentelemetry-collector).**\n\nContent of the `jaeger-values.yaml` file:\n\n```YAML\nhotrod:\n  enabled: true\n  # Switch from the jaeger protocol to OTLP\n  extraArgs:\n    - --otel-exporter=otlp\n  # Set the address of the OpenTelemetry collector endpoint\n  extraEnv:\n    - name: OTEL_EXPORTER_OTLP_ENDPOINT\n      value: http://my-otel-collector-opentelemetry-collector:4318\n```\n'
      if name == 'shenyu':
        return "# shenyu-helm-chart-test\nHelm deployment documentation written for Apache/Shenyu\n\n# Artifact Hub\n\n[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/shenyu-test)](https://artifacthub.io/packages/search?repo=shenyu-test)\n\n\n## Usage-test\n\n[Helm](https://helm.sh) must be installed to use the charts.  Please refer to\nHelm's [documentation](https://helm.sh/docs) to get started.\n\nOnce Helm has been set up correctly, add the repo as follows:\n\nadd repo\n\n> helm repo add shenyu https://erdengk.github.io/shenyu-helm-chart\n\ninstall:\n\n> helm install my-shenyu shenyu/shenyu --version 0.6.2\n\n\n\nhave fun～"
      if name == 'jaeger-operator':
        return "# jaeger-operator\n\n[jaeger-operator](https://github.com/jaegertracing/jaeger-operator) is a Kubernetes operator.\n\n## Install\n\n```console\n$ helm install jaegertracing/jaeger-operator\n```\n\n## Introduction\n\nThis chart bootstraps a jaeger-operator deployment on a [Kubernetes](http://kubernetes.io) cluster using the [Helm](https://helm.sh) package manager.\n\n## Prerequisites\n\n- Kubernetes 1.19+\n- Helm 3\n- cert-manager 1.6.1+ installed, or certificate for webhook service in a secret\n\n## Check compability matrix\nSee the compatibility matrix [here](./COMPATIBILITY.md).\n\n## Installing the Chart\n\nAdd the Jaeger Tracing Helm repository:\n\n```console\n$ helm repo add jaegertracing https://jaegertracing.github.io/helm-charts\n```\n\nTo install the chart with the release name `my-release` in `observability` namespace: \n\n```console\n$ helm install my-release jaegertracing/jaeger-operator -n observability\n```\n\nThe command deploys jaeger-operator on the Kubernetes cluster in the default configuration. The [configuration](#configuration) section lists the parameters that can be configured during installation.\n\n> **Tip**: List all releases using `helm list`\n\n## Uninstalling the Chart\n\nTo uninstall/delete the `my-release` deployment:\n\n```console\n$ helm delete my-release\n```\n\nThe command removes all the Kubernetes components associated with the chart and deletes the release.\n\n## Configuration\n\nThe following table lists the configurable parameters of the jaeger-operator chart and their default values.\n\n| Parameter                  | Description                                                                                                 | Default                         |\n|-:--------------------------|-:-----------------------------------------------------------------------------------------------------------|-:-------------------------------|\n| `serviceExtraLabels`       | Additional labels to jaeger-operator service                                                                | `{}`                            |\n| `extraLabels`              | Additional labels to jaeger-operator deployment                                                             | `{}`                            |\n| `image.repository`         | Controller container image repository                                                                       | `jaegertracing/jaeger-operator` |\n| `image.tag`                | Controller container image tag                                                                              | `1.46.0`                        |\n| `image.pullPolicy`         | Controller container image pull policy                                                                      | `IfNotPresent`                  |\n| `jaeger.create`            | Jaeger instance will be created                                                                             | `false`                         |\n| `jaeger.spec`              | Jaeger instance specification                                                                               | `{}`                            |\n| `rbac.create`              | All required roles and rolebindings will be created                                                         | `true`                          |\n| `serviceAccount.create`    | Service account to use                                                                                      | `true`                          |\n| `rbac.pspEnabled`          | Pod security policy for pod will be created and included in rbac role                                       | `false`                         |\n| `rbac.clusterRole`         | ClusterRole will be used by operator ServiceAccount                                                         | `false`                         |\n| `serviceAccount.name`      | Service account name to use. If not set and create is true, a name is generated using the fullname template | `nil`                           |\n| `extraEnv`                 | Additional environment variables passed to the operator. For example:   name: LOG-LEVEL   value: debug      | `[]`                            |\n| `resources`                | K8s pod resources                                                                                           | `None`                          |\n| `nodeSelector`             | Node labels for pod assignment                                                                              | `{}`                            |\n| `tolerations`              | Toleration labels for pod assignment                                                                        | `[]`                            |\n| `affinity`                 | Affinity settings for pod assignment                                                                        | `{}`                            |\n| `securityContext`          | Security context for pod                                                                                    | `{}`                            |\n| `containerSecurityContext` | Security context for the container                                                                          | `{}`                            |\n| `priorityClassName`        | Priority class name for the pod                                                                             | `None`                          |\n\nSpecify each parameter you'd like to override using a YAML file as described above in the [installation](#installing-the-chart) section.\n\nYou can also specify any non-array parameter using the `--set key=value[,key=value]` argument to `helm install`. For example,\n\n```console\n$ helm install jaegertracing/jaeger-operator --name my-release \\\n    --set rbac.create=false\n```\n\nTo install the chart without creating the CRDs (any files under `chart/crds`) make use of the `--skip-crds` flag. For example,\n\n```console\n$ helm install jaegertracing/jaeger-operator --name my-release \\\n    --skip-crds\n```\n\n## After the Helm Installation\n\n### Creating a new Jaeger instance\n\nThe simplest possible way to install is by creating a YAML file like the following:\n\n```YAML\napiVersion: jaegertracing.io/v1\nkind: Jaeger\nmetadata:\n  name: simplest\n```\n\nThe YAML file can then be used with `kubectl`:\n\n```console\n$ kubectl apply -f simplest.yaml\n```\n\n### Creating a new Jaeger with ElasticSearch\n\nTo do that you need to have an ElasticSearch installed in your Kubernetes cluster or install one using the [Helm Chart](https://github.com/helm/charts/tree/master/incubator/elasticsearch) available for that.\n\nAfter that just deploy the following manifest:\n\n```YAML\n# setup an elasticsearch with `make es`\napiVersion: jaegertracing.io/v1\nkind: Jaeger\nmetadata:\n  name: simple-prod\nspec:\n  strategy: production\n  storage:\n    type: elasticsearch\n    options:\n      es:\n        server-urls: http://elasticsearch:9200\n        username: elastic\n        password: changeme\n```\n\nThe YAML file can then be used with `kubectl`:\n\n```console\n$ kubectl apply -f simple-prod.yaml\n```\n"
      if name == 'velero':
        return "# Velero\n\nVelero is an open source tool to safely backup and restore, perform disaster recovery, and migrate Kubernetes cluster resources and persistent volumes.\n\nVelero has two main components: a CLI, and a server-side Kubernetes deployment.\n\n## Installing the Velero CLI\n\nSee the different options for installing the [Velero CLI](https://velero.io/docs/v1.11/basic-install/#install-the-cli).\n\n## Installing the Velero server\n\n### Installation Requirements\n\nKubernetes v1.16+, because this helm chart uses CustomResourceDefinition `apiextensions.k8s.io/v1`. This API version was introduced in Kubernetes v1.16.\n\n### Velero version\n\nThis helm chart installs Velero version v1.11 https://velero.io/docs/v1.11/. See the [#Upgrading](#upgrading) section for information on how to upgrade from other versions.\n\n### Provider credentials\n\nWhen installing using the Helm chart, the provider's credential information will need to be appended into your values. The easiest way to do this is with the `--set-file` argument, available in Helm 2.10 and higher. See your cloud provider's documentation for the contents and creation of the `credentials-velero` file.\n\n### Installing\n\nThe default configuration values for this chart are listed in values.yaml.\n\nSee Velero's full [official documentation](https://velero.io/docs/v1.11/basic-install/). More specifically, find your provider in the Velero list of [supported providers](https://velero.io/docs/v1.11/supported-providers/) for specific configuration information and examples.\n\n#### Set up Helm\n\nSee the main [README.md](https://github.com/vmware-tanzu/helm-charts#kubernetes-helm-charts-for-vmware-tanzu).\n\n#### Using Helm 3\n\n##### Option 1) CLI commands\n\nNote: You may add the flag `--set cleanUpCRDs=true` if you want to delete the Velero CRDs after deleting a release.\nPlease note that cleaning up CRDs will also delete any CRD instance, such as BackupStorageLocation and VolumeSnapshotLocation, which would have to be reconfigured when reinstalling Velero. The backup data in object storage will not be deleted, even though the backup instances in the cluster will.\n\nSpecify the necessary values using the --set key=value[,key=value] argument to helm install. For example,\n\n```bash\nhelm install velero vmware-tanzu/velero \\\n--namespace <YOUR NAMESPACE> \\\n--create-namespace \\\n--set-file credentials.secretContents.cloud=<FULL PATH TO FILE> \\\n--set configuration.backupStorageLocation[0].name=<BACKUP STORAGE LOCATION NAME> \\\n--set configuration.backupStorageLocation[0].provider=<PROVIDER NAME> \\\n--set configuration.backupStorageLocation[0].bucket=<BUCKET NAME> \\\n--set configuration.backupStorageLocation[0].config.region=<REGION> \\\n--set configuration.volumeSnapshotLocation[0].name=<VOLUME SNAPSHOT LOCATION NAME> \\\n--set configuration.volumeSnapshotLocation[0].provider=<PROVIDER NAME> \\\n--set configuration.volumeSnapshotLocation[0].config.region=<REGION> \\\n--set initContainers[0].name=velero-plugin-for-<PROVIDER NAME> \\\n--set initContainers[0].image=velero/velero-plugin-for-<PROVIDER NAME>:<PROVIDER PLUGIN TAG> \\\n--set initContainers[0].volumeMounts[0].mountPath=/target \\\n--set initContainers[0].volumeMounts[0].name=plugins\n```\n\nUsers of zsh might need to put quotes around key/value pairs.\n\n##### Option 2) YAML file\n\nAdd/update the necessary values by changing the values.yaml from this repository, then run:\n\n```bash\nhelm install vmware-tanzu/velero --namespace <YOUR NAMESPACE> -f values.yaml --generate-name\n```\n##### Upgrade the configuration\n\nIf a value needs to be added or changed, you may do so with the `upgrade` command. An example:\n\n```bash\nhelm upgrade <RELEASE NAME> vmware-tanzu/velero --namespace <YOUR NAMESPACE> --reuse-values --set configuration.backupStorageLocation[0].provider=<NEW PROVIDER>\n```\n\n#### Using Helm 2\n\nWe're no longer supporting Helm v2 since it was deprecated in November 2020.\n\n##### Upgrade the configuration\n\nIf a value needs to be added or changed, you may do so with the `upgrade` command. An example:\n\n```bash\nhelm upgrade vmware-tanzu/velero <RELEASE NAME> --reuse-values --set configuration.backupStorageLocation[0].provider=<NEW PROVIDER>\n```\n\n## Upgrading\n\n### Upgrading to v1.11\n\nThe [instructions found here](https://velero.io/docs/v1.11/upgrade-to-1.11/) will assist you in upgrading from version v1.10.x to v1.11.\n\n### Upgrading to v1.10\n\nThe [instructions found here](https://velero.io/docs/v1.10/upgrade-to-1.10/) will assist you in upgrading from version v1.9.x to v1.10.\n\n### Upgrading to v1.9\n\nThe [instructions found here](https://velero.io/docs/v1.9/upgrade-to-1.9/) will assist you in upgrading from version v1.8.x to v1.9.\n\n### Upgrading to v1.8\n\nThe [instructions found here](https://velero.io/docs/v1.8/upgrade-to-1.8/) will assist you in upgrading from version v1.7.x to v1.8.\n\n### Upgrading to v1.7\n\nThe [instructions found here](https://velero.io/docs/v1.7/upgrade-to-1.7/) will assist you in upgrading from version v1.6.x to v1.7.\n\n### Upgrading to v1.6\n\nThe [instructions found here](https://velero.io/docs/v1.6/upgrade-to-1.6/) will assist you in upgrading from version v1.5.x to v1.6.\n\n### Upgrading to v1.5\n\nThe [instructions found here](https://velero.io/docs/v1.5/upgrade-to-1.5/) will assist you in upgrading from version v1.4.x to v1.5.\n\n### Upgrading to v1.4\n\nThe [instructions found here](https://velero.io/docs/v1.4/upgrade-to-1.4/) will assist you in upgrading from version v1.3.x to v1.4.\n\n### Upgrading to v1.3.1\n\nThe [instructions found here](https://velero.io/docs/v1.3.1/upgrade-to-1.3/) will assist you in upgrading from version v1.2.0 or v1.3.0 to v1.3.1.\n\n### Upgrading to v1.2.0\n\nThe [instructions found here](https://velero.io/docs/v1.2.0/upgrade-to-1.2/) will assist you in upgrading from version v1.0.0 or v1.1.0 to v1.2.0.\n\n### Upgrading to v1.1.0\n\nThe [instructions found here](https://velero.io/docs/v1.1.0/upgrade-to-1.1/) will assist you in upgrading from version v1.0.0 to v1.1.0.\n\n## Uninstall Velero\n\nNote: when you uninstall the Velero server, all backups remain untouched.\n\n### Using Helm 3\n\n```bash\nhelm uninstall <RELEASE NAME> -n <YOUR NAMESPACE>\n```\n### Note\nSince from velero v1.10.0, it has supported both Restic and Kopia to do file-system level backup and restore, some configuration that contains the keyword Restic is not suitable anymore, which means from chart version 3.0.0 is not backward compatible, and we've done a configure filed name validation.\n"
      if name == "keda":
        return '<p align="center"><img src="https://raw.githubusercontent.com/kedacore/keda/main/images/keda-logo-transparent.png" width="300"/></p>\r\n<p style="font-size: 25px" align="center"><b>Kubernetes-based Event Driven Autoscaling</b></p>\r\n\r\nKEDA allows for fine grained autoscaling (including to/from zero) for event driven Kubernetes workloads.  KEDA serves as a Kubernetes Metrics Server and allows users to define autoscaling rules using a dedicated Kubernetes custom resource definition.\r\n\r\nKEDA can run on both the cloud and the edge, integrates natively with Kubernetes components such as the Horizontal Pod Autoscaler, and has no external dependencies.\r\n\r\n---\r\n<p align="center">\r\nWe are a Cloud Native Computing Foundation (CNCF) incubation project.\r\n\r\n<img src="https://raw.githubusercontent.com/kedacore/keda/main/images/logo-cncf.svg" height="75px">\r\n</p>\r\n\r\n---\r\n\r\n## TL;DR\r\n\r\n```console\r\nhelm repo add kedacore https://kedacore.github.io/charts\r\nhelm repo update\r\n\r\nkubectl create namespace keda\r\nhelm install keda kedacore/keda --namespace keda --version 2.10.2\r\n```\r\n\r\n## Introduction\r\n\r\nThis chart bootstraps KEDA infrastructure on a Kubernetes cluster using the Helm package manager.\r\n\r\nAs part of that, it will install all the required Custom Resource Definitions (CRD).\r\n\r\n## Installing the Chart\r\n\r\nTo install the chart with the release name `keda`:\r\n\r\n```console\r\n$ kubectl create namespace keda\r\n$ helm install keda kedacore/keda --namespace keda --version 2.11.1\r\n```\r\n\r\n## Uninstalling the Chart\r\n\r\nTo uninstall/delete the `keda` Helm chart:\r\n\r\n```console\r\nhelm uninstall keda\r\n```\r\n\r\nThe command removes all the Kubernetes components associated with the chart and deletes the release.\r\n\r\n## Configuration\r\n\r\nThe following table lists the configurable parameters of the KEDA chart and\r\ntheir default values.\r\n\r\n### General parameters\r\n\r\n| Parameter | Type | Default | Description |\r\n|-----------|------|---------|-------------|\r\n| `additionalAnnotations` | object | `{}` | Custom annotations to add into metadata |\r\n| `additionalLabels` | object | `{}` | Custom labels to add into metadata |\r\n| `affinity` | object | `{}` | [Affinity] for pod scheduling for both KEDA operator and Metrics API Server |\r\n| `asciiArt` | bool | `true` | Capability to turn on/off ASCII art in Helm installation notes |\r\n| `certificates.autoGenerated` | bool | `true` | Enables the self generation for KEDA TLS certificates inside KEDA operator |\r\n| `certificates.certManager.caSecretName` | string | `"kedaorg-ca"` | Secret name where the CA is stored (generatedby cert-manager or user given) |\r\n| `certificates.certManager.enabled` | bool | `false` | Enables Cert-manager for certificate management |\r\n| `certificates.certManager.generateCA` | bool | `true` | Generates a self-signed CA with Cert-manager. If generateCA is false, the secret with the CA has to be annotated with `cert-manager.io/allow-direct-injection: "true"` |\r\n| `certificates.certManager.secretTemplate` | object | `{}` | Add labels/annotations to secrets created by Certificate resources [docs](https://cert-manager.io/docs/usage/certificate/#creating-certificate-resources) |\r\n| `certificates.mountPath` | string | `"/certs"` | Path where KEDA TLS certificates are mounted |\r\n| `certificates.secretName` | string | `"kedaorg-certs"` | Secret name to be mounted with KEDA TLS certificates |\r\n| `clusterDomain` | string | `"cluster.local"` | Kubernetes cluster domain |\r\n| `crds.install` | bool | `true` | Defines whether the KEDA CRDs have to be installed or not. |\r\n| `env` | list | `[]` | Additional environment variables that will be passed onto all KEDA components |\r\n| `extraObjects` | list | `[]` | Array of extra K8s manifests to deploy |\r\n| `grpcTLSCertsSecret` | string | `""` | Set this if you are using an external scaler and want to communicate over TLS (recommended). This variable holds the name of the secret that will be mounted to the /grpccerts path on the Pod |\r\n| `hashiCorpVaultTLS` | string | `""` | Set this if you are using HashiCorp Vault and want to communicate over TLS (recommended). This variable holds the name of the secret that will be mounted to the /vault path on the Pod |\r\n| `http.keepAlive.enabled` | bool | `true` | Enable HTTP connection keep alive |\r\n| `http.minTlsVersion` | string | `"TLS12"` | The minimum TLS version to use for all scalers that use raw HTTP clients (some scalers use SDKs to access target services. These have built-in HTTP clients, and this value does not necessarily apply to them) |\r\n| `http.timeout` | int | `3000` | The default HTTP timeout to use for all scalers that use raw HTTP clients (some scalers use SDKs to access target services. These have built-in HTTP clients, and the timeout does not necessarily apply to them) |\r\n| `image.pullPolicy` | string | `"Always"` | Image pullPolicy for all KEDA components |\r\n| `imagePullSecrets` | list | `[]` | Name of secret to use to pull images to use to pull Docker images |\r\n| `nodeSelector` | object | `{}` | Node selector for pod scheduling ([docs](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)) |\r\n| `podIdentity.activeDirectory.identity` | string | `""` | Identity in Azure Active Directory to use for Azure pod identity |\r\n| `podIdentity.aws.irsa.audience` | string | `"sts.amazonaws.com"` | Sets the token audience for IRSA. This will be set as an annotation on the KEDA service account. |\r\n| `podIdentity.aws.irsa.enabled` | bool | `false` | Specifies whether [AWS IAM Roles for Service Accounts (IRSA)](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) is to be enabled or not. |\r\n| `podIdentity.aws.irsa.roleArn` | string | `""` | Set to the value of the ARN of an IAM role with a web identity provider. This will be set as an annotation on the KEDA service account. |\r\n| `podIdentity.aws.irsa.stsRegionalEndpoints` | string | `"true"` | Sets the use of an STS regional endpoint instead of global. Recommended to use regional endpoint in almost all cases. This will be set as an annotation on the KEDA service account. |\r\n| `podIdentity.aws.irsa.tokenExpiration` | int | `86400` | Set to the value of the service account token expiration duration. This will be set as an annotation on the KEDA service account. |\r\n| `podIdentity.azureWorkload.clientId` | string | `""` | Id of Azure Active Directory Client to use for authentication with Azure Workload Identity. ([docs](https://keda.sh/docs/concepts/authentication/#azure-workload-identity)) |\r\n| `podIdentity.azureWorkload.enabled` | bool | `false` | Set to true to enable Azure Workload Identity usage. See https://keda.sh/docs/concepts/authentication/#azure-workload-identity This will be set as a label on the KEDA service account. |\r\n| `podIdentity.azureWorkload.tenantId` | string | `""` | Id Azure Active Directory Tenant to use for authentication with for Azure Workload Identity. ([docs](https://keda.sh/docs/concepts/authentication/#azure-workload-identity)) |\r\n| `podIdentity.azureWorkload.tokenExpiration` | int | `3600` | Duration in seconds to automatically expire tokens for the service account. ([docs](https://keda.sh/docs/concepts/authentication/#azure-workload-identity)) |\r\n| `podIdentity.gcp.enabled` | bool | `false` | Set to true to enable GCP Workload Identity. See https://keda.sh/docs/2.10/authentication-providers/gcp-workload-identity/ This will be set as a annotation on the KEDA service account. |\r\n| `podIdentity.gcp.gcpIAMServiceAccount` | string | `""` | GCP IAM Service Account Email which you would like to use for workload identity. |\r\n| `podSecurityContext` | object | [See below](#KEDA-is-secure-by-default) | [Pod security context] for all pods |\r\n| `priorityClassName` | string | `""` | priorityClassName for all KEDA components |\r\n| `rbac.aggregateToDefaultRoles` | bool | `false` | Specifies whether RBAC for CRDs should be [aggregated](https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles) to default roles (view, edit, admin) |\r\n| `rbac.create` | bool | `true` | Specifies whether RBAC should be used |\r\n| `securityContext` | object | [See below](#KEDA-is-secure-by-default) | [Security context] for all containers |\r\n| `serviceAccount.annotations` | object | `{}` | Annotations to add to the service account |\r\n| `serviceAccount.automountServiceAccountToken` | bool | `true` | Specifies whether a service account should automount API-Credentials |\r\n| `serviceAccount.create` | bool | `true` | Specifies whether a service account should be created |\r\n| `serviceAccount.name` | string | `"keda-operator"` | The name of the service account to use. If not set and create is true, a name is generated using the fullname template |\r\n| `tolerations` | list | `[]` | Tolerations for pod scheduling ([docs](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)) |\r\n| `watchNamespace` | string | `""` | Defines Kubernetes namespaces to watch to scale their workloads. Default watches all namespaces |\r\n\r\n### Operator\r\n\r\n| Parameter | Type | Default | Description |\r\n|-----------|------|---------|-------------|\r\n| `extraArgs.keda` | object | `{}` | Additional KEDA Operator container arguments |\r\n| `image.keda.repository` | string | `"ghcr.io/kedacore/keda"` | Image name of KEDA operator |\r\n| `image.keda.tag` | string | `""` | Image tag of KEDA operator. Optional, given app version of Helm chart is used by default |\r\n| `logging.operator.format` | string | `"console"` | Logging format for KEDA Operator. allowed values: `json` or `console` |\r\n| `logging.operator.level` | string | `"info"` | Logging level for KEDA Operator. allowed values: `debug`, `info`, `error`, or an integer value greater than 0, specified as string |\r\n| `logging.operator.timeEncoding` | string | `"rfc3339"` | Logging time encoding for KEDA Operator. allowed values are `epoch`, `millis`, `nano`, `iso8601`, `rfc3339` or `rfc3339nano` |\r\n| `operator.affinity` | object | `{}` | [Affinity] for pod scheduling for KEDA operator. Takes precedence over the `affinity` field |\r\n| `operator.name` | string | `"keda-operator"` | Name of the KEDA operator |\r\n| `operator.replicaCount` | int | `1` | Capability to configure the number of replicas for KEDA operator. While you can run more replicas of our operator, only one operator instance will be the leader and serving traffic. You can run multiple replicas, but they will not improve the performance of KEDA, it could only reduce downtime during a failover. Learn more in [our documentation](https://keda.sh/docs/latest/operate/cluster/#high-availability). |\r\n| `permissions.operator.restrict.secret` | bool | `false` | Restrict Secret Access for KEDA operator |\r\n| `podAnnotations.keda` | object | `{}` | Pod annotations for KEDA operator |\r\n| `podDisruptionBudget.operator` | object | `{}` | Capability to configure [Pod Disruption Budget] |\r\n| `podLabels.keda` | object | `{}` | Pod labels for KEDA operator |\r\n| `podSecurityContext.operator` | object | [See below](#KEDA-is-secure-by-default) | [Pod security context] of the KEDA operator pod |\r\n| `prometheus.operator.enabled` | bool | `false` | Enable KEDA Operator prometheus metrics expose |\r\n| `prometheus.operator.podMonitor.additionalLabels` | object | `{}` | Additional labels to add for KEDA Operator using podMonitor crd (prometheus operator) |\r\n| `prometheus.operator.podMonitor.enabled` | bool | `false` | Enables PodMonitor creation for the Prometheus Operator |\r\n| `prometheus.operator.podMonitor.interval` | string | `""` | Scraping interval for KEDA Operator using podMonitor crd (prometheus operator) |\r\n| `prometheus.operator.podMonitor.namespace` | string | `""` | Scraping namespace for KEDA Operator using podMonitor crd (prometheus operator) |\r\n| `prometheus.operator.podMonitor.relabelings` | list | `[]` | List of expressions that define custom relabeling rules for KEDA Operator podMonitor crd (prometheus operator) |\r\n| `prometheus.operator.podMonitor.scrapeTimeout` | string | `""` | Scraping timeout for KEDA Operator using podMonitor crd (prometheus operator) |\r\n| `prometheus.operator.port` | int | `8080` | Port used for exposing KEDA Operator prometheus metrics |\r\n| `prometheus.operator.prometheusRules.additionalLabels` | object | `{}` | Additional labels to add for KEDA Operator using prometheusRules crd (prometheus operator) |\r\n| `prometheus.operator.prometheusRules.alerts` | list | `[]` | Additional alerts to add for KEDA Operator using prometheusRules crd (prometheus operator) |\r\n| `prometheus.operator.prometheusRules.enabled` | bool | `false` | Enables PrometheusRules creation for the Prometheus Operator |\r\n| `prometheus.operator.prometheusRules.namespace` | string | `""` | Scraping namespace for KEDA Operator using prometheusRules crd (prometheus operator) |\r\n| `prometheus.operator.serviceMonitor.additionalLabels` | object | `{}` | Additional labels to add for metric server using ServiceMonitor crd (prometheus operator) |\r\n| `prometheus.operator.serviceMonitor.enabled` | bool | `false` | Enables ServiceMonitor creation for the Prometheus Operator |\r\n| `prometheus.operator.serviceMonitor.interval` | string | `""` | Interval at which metrics should be scraped If not specified Prometheus’ global scrape interval is used. |\r\n| `prometheus.operator.serviceMonitor.jobLabel` | string | `""` | JobLabel selects the label from the associated Kubernetes service which will be used as the job label for all metrics. [ServiceMonitor Spec] |\r\n| `prometheus.operator.serviceMonitor.podTargetLabels` | list | `[]` | PodTargetLabels transfers labels on the Kubernetes `Pod` onto the created metrics |\r\n| `prometheus.operator.serviceMonitor.port` | string | `"metrics"` | Name of the service port this endpoint refers to. Mutually exclusive with targetPort |\r\n| `prometheus.operator.serviceMonitor.relabelings` | list | `[]` | List of expressions that define custom relabeling rules for metric server ServiceMonitor crd (prometheus operator). [RelabelConfig Spec] |\r\n| `prometheus.operator.serviceMonitor.relabellings` | list | `[]` | DEPRECATED. List of expressions that define custom relabeling rules for metric server ServiceMonitor crd (prometheus operator). [RelabelConfig Spec] |\r\n| `prometheus.operator.serviceMonitor.scrapeTimeout` | string | `""` | Timeout after which the scrape is ended If not specified, the Prometheus global scrape timeout is used unless it is less than Interval in which the latter is used |\r\n| `prometheus.operator.serviceMonitor.targetLabels` | list | `[]` | TargetLabels transfers labels from the Kubernetes `Service` onto the created metrics |\r\n| `prometheus.operator.serviceMonitor.targetPort` | string | `""` | Name or number of the target port of the Pod behind the Service, the port must be specified with container port property. Mutually exclusive with port |\r\n| `resources.operator` | object | `{"limits":{"cpu":1,"memory":"1000Mi"},"requests":{"cpu":"100m","memory":"100Mi"}}` | Manage [resource request & limits] of KEDA operator pod |\r\n| `securityContext.operator` | object | [See below](#KEDA-is-secure-by-default) | [Security context] of the operator container |\r\n| `topologySpreadConstraints.operator` | list | `[]` | [Pod Topology Constraints] of KEDA operator pod |\r\n| `upgradeStrategy.operator` | object | `{}` | Capability to configure [Deployment upgrade strategy] for operator |\r\n| `volumes.keda.extraVolumeMounts` | list | `[]` | Extra volume mounts for KEDA deployment |\r\n| `volumes.keda.extraVolumes` | list | `[]` | Extra volumes for KEDA deployment |\r\n\r\n### Metrics server\r\n\r\n| Parameter | Type | Default | Description |\r\n|-----------|------|---------|-------------|\r\n| `extraArgs.metricsAdapter` | object | `{}` | Additional Metrics Adapter container arguments |\r\n| `image.metricsApiServer.repository` | string | `"ghcr.io/kedacore/keda-metrics-apiserver"` | Image name of KEDA Metrics API Server |\r\n| `image.metricsApiServer.tag` | string | `""` | Image tag of KEDA Metrics API Server. Optional, given app version of Helm chart is used by default |\r\n| `logging.metricServer.level` | int | `0` | Logging level for Metrics Server. allowed values: `0` for info, `4` for debug, or an integer value greater than 0, specified as string |\r\n| `metricsServer.affinity` | object | `{}` | [Affinity] for pod scheduling for Metrics API Server. Takes precedence over the `affinity` field |\r\n| `metricsServer.dnsPolicy` | string | `"ClusterFirst"` | Defined the DNS policy for the metric server |\r\n| `metricsServer.replicaCount` | int | `1` | Capability to configure the number of replicas for KEDA metric server. While you can run more replicas of our metric server, only one instance will used and serve traffic. You can run multiple replicas, but they will not improve the performance of KEDA, it could only reduce downtime during a failover. Learn more in [our documentation](https://keda.sh/docs/latest/operate/cluster/#high-availability). |\r\n| `metricsServer.useHostNetwork` | bool | `false` | Enable metric server to use host network |\r\n| `permissions.metricServer.restrict.secret` | bool | `false` | Restrict Secret Access for Metrics Server |\r\n| `podAnnotations.metricsAdapter` | object | `{}` | Pod annotations for KEDA Metrics Adapter |\r\n| `podDisruptionBudget.metricServer` | object | `{}` | Capability to configure [Pod Disruption Budget] |\r\n| `podLabels.metricsAdapter` | object | `{}` | Pod labels for KEDA Metrics Adapter |\r\n| `podSecurityContext.metricServer` | object | [See below](#KEDA-is-secure-by-default) | [Pod security context] of the KEDA metrics apiserver pod |\r\n| `prometheus.metricServer.enabled` | bool | `false` | Enable metric server Prometheus metrics expose |\r\n| `prometheus.metricServer.podMonitor.additionalLabels` | object | `{}` | Additional labels to add for metric server using podMonitor crd (prometheus operator) |\r\n| `prometheus.metricServer.podMonitor.enabled` | bool | `false` | Enables PodMonitor creation for the Prometheus Operator |\r\n| `prometheus.metricServer.podMonitor.interval` | string | `""` | Scraping interval for metric server using podMonitor crd (prometheus operator) |\r\n| `prometheus.metricServer.podMonitor.namespace` | string | `""` | Scraping namespace for metric server using podMonitor crd (prometheus operator) |\r\n| `prometheus.metricServer.podMonitor.relabelings` | list | `[]` | List of expressions that define custom relabeling rules for metric server podMonitor crd (prometheus operator) |\r\n| `prometheus.metricServer.podMonitor.scrapeTimeout` | string | `""` | Scraping timeout for metric server using podMonitor crd (prometheus operator) |\r\n| `prometheus.metricServer.port` | int | `8080` | HTTP port used for exposing metrics server prometheus metrics |\r\n| `prometheus.metricServer.portName` | string | `"metrics"` | HTTP port name for exposing metrics server prometheus metrics |\r\n| `prometheus.metricServer.serviceMonitor.additionalLabels` | object | `{}` | Additional labels to add for metric server using ServiceMonitor crd (prometheus operator) |\r\n| `prometheus.metricServer.serviceMonitor.enabled` | bool | `false` | Enables ServiceMonitor creation for the Prometheus Operator |\r\n| `prometheus.metricServer.serviceMonitor.interval` | string | `""` | Interval at which metrics should be scraped If not specified Prometheus’ global scrape interval is used. |\r\n| `prometheus.metricServer.serviceMonitor.jobLabel` | string | `""` | JobLabel selects the label from the associated Kubernetes service which will be used as the job label for all metrics. [ServiceMonitor Spec] |\r\n| `prometheus.metricServer.serviceMonitor.podTargetLabels` | list | `[]` | PodTargetLabels transfers labels on the Kubernetes `Pod` onto the created metrics |\r\n| `prometheus.metricServer.serviceMonitor.port` | string | `"metrics"` | Name of the service port this endpoint refers to. Mutually exclusive with targetPort |\r\n| `prometheus.metricServer.serviceMonitor.relabelings` | list | `[]` | List of expressions that define custom relabeling rules for metric server ServiceMonitor crd (prometheus operator). [RelabelConfig Spec] |\r\n| `prometheus.metricServer.serviceMonitor.relabellings` | list | `[]` | DEPRECATED. List of expressions that define custom relabeling rules for metric server ServiceMonitor crd (prometheus operator). [RelabelConfig Spec] |\r\n| `prometheus.metricServer.serviceMonitor.scrapeTimeout` | string | `""` | Timeout after which the scrape is ended If not specified, the Prometheus global scrape timeout is used unless it is less than Interval in which the latter is used |\r\n| `prometheus.metricServer.serviceMonitor.targetLabels` | list | `[]` | TargetLabels transfers labels from the Kubernetes `Service` onto the created metrics |\r\n| `prometheus.metricServer.serviceMonitor.targetPort` | string | `""` | Name or number of the target port of the Pod behind the Service, the port must be specified with container port property. Mutually exclusive with port |\r\n| `resources.metricServer` | object | `{"limits":{"cpu":1,"memory":"1000Mi"},"requests":{"cpu":"100m","memory":"100Mi"}}` | Manage [resource request & limits] of KEDA metrics apiserver pod |\r\n| `securityContext.metricServer` | object | [See below](#KEDA-is-secure-by-default) | [Security context] of the metricServer container |\r\n| `service.annotations` | object | `{}` | Annotations to add the KEDA Metric Server service |\r\n| `service.portHttps` | int | `443` | HTTPS port for KEDA Metric Server service |\r\n| `service.portHttpsTarget` | int | `6443` | HTTPS port for KEDA Metric Server container |\r\n| `service.type` | string | `"ClusterIP"` | KEDA Metric Server service type |\r\n| `topologySpreadConstraints.metricsServer` | list | `[]` | [Pod Topology Constraints] of KEDA metrics apiserver pod |\r\n| `upgradeStrategy.metricsApiServer` | object | `{}` | Capability to configure [Deployment upgrade strategy] for Metrics Api Server |\r\n| `volumes.metricsApiServer.extraVolumeMounts` | list | `[]` | Extra volume mounts for metric server deployment |\r\n| `volumes.metricsApiServer.extraVolumes` | list | `[]` | Extra volumes for metric server deployment |\r\n\r\n### Admission Webhooks\r\n\r\n| Parameter | Type | Default | Description |\r\n|-----------|------|---------|-------------|\r\n| `image.webhooks.repository` | string | `"ghcr.io/kedacore/keda-admission-webhooks"` | Image name of KEDA admission-webhooks |\r\n| `image.webhooks.tag` | string | `""` | Image tag of KEDA admission-webhooks . Optional, given app version of Helm chart is used by default |\r\n| `logging.webhooks.format` | string | `"console"` | Logging format for KEDA Admission webhooks. allowed values: `json` or `console` |\r\n| `logging.webhooks.level` | string | `"info"` | Logging level for KEDA Operator. allowed values: `debug`, `info`, `error`, or an integer value greater than 0, specified as string |\r\n| `logging.webhooks.timeEncoding` | string | `"rfc3339"` | Logging time encoding for KEDA Operator. allowed values are `epoch`, `millis`, `nano`, `iso8601`, `rfc3339` or `rfc3339nano` |\r\n| `podAnnotations.webhooks` | object | `{}` | Pod annotations for KEDA Admission webhooks |\r\n| `podDisruptionBudget.webhooks` | object | `{}` | Capability to configure [Pod Disruption Budget] |\r\n| `podLabels.webhooks` | object | `{}` | Pod labels for KEDA Admission webhooks |\r\n| `podSecurityContext.webhooks` | object | [See below](#KEDA-is-secure-by-default) | [Pod security context] of the KEDA admission webhooks |\r\n| `prometheus.webhooks.enabled` | bool | `false` | Enable KEDA admission webhooks prometheus metrics expose |\r\n| `prometheus.webhooks.port` | int | `8080` | Port used for exposing KEDA admission webhooks prometheus metrics |\r\n| `prometheus.webhooks.prometheusRules.additionalLabels` | object | `{}` | Additional labels to add for KEDA admission webhooks using prometheusRules crd (prometheus operator) |\r\n| `prometheus.webhooks.prometheusRules.alerts` | list | `[]` | Additional alerts to add for KEDA admission webhooks using prometheusRules crd (prometheus operator) |\r\n| `prometheus.webhooks.prometheusRules.enabled` | bool | `false` | Enables PrometheusRules creation for the Prometheus Operator |\r\n| `prometheus.webhooks.prometheusRules.namespace` | string | `""` | Scraping namespace for KEDA admission webhooks using prometheusRules crd (prometheus operator) |\r\n| `prometheus.webhooks.serviceMonitor.additionalLabels` | object | `{}` | Additional labels to add for metric server using ServiceMonitor crd (prometheus operator) |\r\n| `prometheus.webhooks.serviceMonitor.enabled` | bool | `false` | Enables ServiceMonitor creation for the Prometheus webhooks |\r\n| `prometheus.webhooks.serviceMonitor.interval` | string | `""` | Interval at which metrics should be scraped If not specified Prometheus’ global scrape interval is used. |\r\n| `prometheus.webhooks.serviceMonitor.jobLabel` | string | `""` | jobLabel selects the label from the associated Kubernetes service which will be used as the job label for all metrics. [ServiceMonitor Spec] |\r\n| `prometheus.webhooks.serviceMonitor.podTargetLabels` | list | `[]` | PodTargetLabels transfers labels on the Kubernetes `Pod` onto the created metrics |\r\n| `prometheus.webhooks.serviceMonitor.port` | string | `"metrics"` | Name of the service port this endpoint refers to. Mutually exclusive with targetPort |\r\n| `prometheus.webhooks.serviceMonitor.relabelings` | list | `[]` | List of expressions that define custom relabeling rules for metric server ServiceMonitor crd (prometheus operator). [RelabelConfig Spec] |\r\n| `prometheus.webhooks.serviceMonitor.relabellings` | list | `[]` | DEPRECATED. List of expressions that define custom relabeling rules for metric server ServiceMonitor crd (prometheus operator). [RelabelConfig Spec] |\r\n| `prometheus.webhooks.serviceMonitor.scrapeTimeout` | string | `""` | Timeout after which the scrape is ended If not specified, the Prometheus global scrape timeout is used unless it is less than Interval in which the latter is used |\r\n| `prometheus.webhooks.serviceMonitor.targetLabels` | list | `[]` | TargetLabels transfers labels from the Kubernetes `Service` onto the created metrics |\r\n| `prometheus.webhooks.serviceMonitor.targetPort` | string | `""` | Name or number of the target port of the Pod behind the Service, the port must be specified with container port property. Mutually exclusive with port |\r\n| `resources.webhooks` | object | `{"limits":{"cpu":"50m","memory":"100Mi"},"requests":{"cpu":"10m","memory":"10Mi"}}` | Manage [resource request & limits] of KEDA admission webhooks pod |\r\n| `securityContext.webhooks` | object | [See below](#KEDA-is-secure-by-default) | [Security context] of the admission webhooks container |\r\n| `topologySpreadConstraints.webhooks` | list | `[]` | [Pod Topology Constraints] of KEDA admission webhooks pod |\r\n| `upgradeStrategy.webhooks` | object | `{}` | Capability to configure [Deployment upgrade strategy] for Admission webhooks |\r\n| `volumes.webhooks.extraVolumeMounts` | list | `[]` | Extra volume mounts for admission webhooks deployment |\r\n| `volumes.webhooks.extraVolumes` | list | `[]` | Extra volumes for admission webhooks deployment |\r\n| `webhooks.affinity` | object | `{}` | [Affinity] for pod scheduling for KEDA admission webhooks. Takes precedence over the `affinity` field |\r\n| `webhooks.enabled` | bool | `true` | Enable admission webhooks (this feature option will be removed in v2.12) |\r\n| `webhooks.failurePolicy` | string | `"Ignore"` | [Failure policy](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#failure-policy) to use with KEDA admission webhooks |\r\n| `webhooks.healthProbePort` | int | `8081` | Port number to use for KEDA admission webhooks health probe |\r\n| `webhooks.name` | string | `"keda-admission-webhooks"` | Name of the KEDA admission webhooks |\r\n| `webhooks.port` | string | `""` | Port number to use for KEDA admission webhooks. Default is 9443. |\r\n| `webhooks.replicaCount` | int | `1` | Capability to configure the number of replicas for KEDA admission webhooks |\r\n| `webhooks.useHostNetwork` | bool | `false` | Enable webhook to use host network, this is required on EKS with custom CNI |\r\n\r\nSpecify each parameter using the `--set key=value[,key=value]` argument to\r\n`helm install`. For example:\r\n\r\n```console\r\n$ helm install keda kedacore/keda --namespace keda \\\r\n               --set image.keda.tag=<different tag from app version> \\\r\n               --set image.metricsApiServer.tag=<different tag from app version> \\\r\n               --set image.webhooks.tag=<different tag from app version>\r\n```\r\n\r\nAlternatively, a YAML file that specifies the values for the above parameters can\r\nbe provided while installing the chart. For example,\r\n\r\n```console\r\nhelm install keda kedacore/keda --namespace keda -f values.yaml\r\n```\r\n\r\n## KEDA is secure by default\r\n\r\nOur default configuration strives to be as secure as possible. Because of that, KEDA will run as non-root and be secure-by-default:\r\n```yaml\r\nsecurityContext:\r\n  operator:\r\n    capabilities:\r\n      drop:\r\n      - ALL\r\n    allowPrivilegeEscalation: false\r\n    readOnlyRootFilesystem: true\r\n    seccompProfile:\r\n      type: RuntimeDefault\r\n  metricServer:\r\n    capabilities:\r\n      drop:\r\n      - ALL\r\n    allowPrivilegeEscalation: false\r\n    ## Metrics server needs to write the self-signed cert. See FAQ for discussion of options.\r\n    # readOnlyRootFilesystem: true\r\n    seccompProfile:\r\n      type: RuntimeDefault\r\n  webhooks:\r\n    capabilities:\r\n      drop:\r\n      - ALL\r\n    allowPrivilegeEscalation: false\r\n    readOnlyRootFilesystem: true\r\n    seccompProfile:\r\n      type: RuntimeDefault\r\n\r\npodSecurityContext:\r\n  operator:\r\n    runAsNonRoot: true\r\n  metricServer:\r\n    runAsNonRoot: true\r\n  webhooks:\r\n    runAsNonRoot: true\r\n```\r\n\r\n----------------------------------------------\r\nAutogenerated from chart metadata using [helm-docs](https://github.com/norwoodj/helm-docs)\r\n\r\n[Affinity]: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/\r\n[Deployment upgrade strategy]: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy\r\n[GCP Workload Identity]: https://keda.sh/docs/2.10/authentication-providers/gcp-workload-identity/\r\n[Pod Disruption Budget]: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\r\n[Pod security context]: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\r\n[Security context]: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container\r\n[Pod Topology Constraints]: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\r\n[RelabelConfig Spec]: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.RelabelConfig\r\n[resource request & limits]: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\r\n[ServiceMonitor Spec]: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.ServiceMonitor\r\n'
      if name == "keda-add-ons-http":
        return '<p align="center"><img src="https://github.com/kedacore/keda/raw/main/images/logos/keda-word-colour.png" width="300"/></p>\n\n<p style="font-size: 25px" align="center"><b>Kubernetes-based Event Driven Autoscaling - HTTP Add-On</b></p>\n<p style="font-size: 25px" align="center">\n\nThe KEDA HTTP Add On allows Kubernetes users to automatically scale their HTTP servers up and down (including to/from zero) based on incoming HTTP traffic. Please see our [use cases document](./docs/use_cases.md) to learn more about how and why you would use this project.\n\n|  **Alpha - Not for production** |\n|---------------------------------------------|\n| ⚠ The HTTP add-on is in [experimental stage](https://github.com/kedacore/keda/issues/538) and not ready for production. <br /><br />It is provided as-is without support.\n\n>This codebase moves very quickly. We can\'t currently guarantee that any part of it will work. Neither the complete feature set nor known issues may be fully documented. Similarly, issues filed against this project may not be responded to quickly or at all. **We will release and announce a beta release of this project**, and after we do that, we will document and respond to issues properly.\n\n## Walkthrough\n\nAlthough this is an **alpha release** project right now, we have prepared a walkthrough document that with instructions on getting started for basic usage.\n\nSee that document at [docs/walkthrough.md](https://github.com/kedacore/http-add-on/tree/main/docs/walkthrough.md)\n\n## Design\n\nThe HTTP add-on is composed of multiple mostly independent components. This design was chosen to allow for highly\ncustomizable installations while allowing us to ship reasonable defaults.\n\n- We have written a complete design document. Please see it at [docs/design.md](https://github.com/kedacore/http-add-on/tree/main/docs/design.md).\n- For more context on the design, please see our [scope document](https://github.com/kedacore/http-add-on/tree/main/docs/scope.md).\n- If you have further questions about the project, please see our [FAQ document](https://github.com/kedacore/http-add-on/tree/main/docs/faq.md).\n\n## Installation\n\nPlease see the [complete installation instructions](https://github.com/kedacore/http-add-on/tree/main/docs/install.md).\n\n## Contributing\n\nPlease see the [contributing documentation for all instructions](https://github.com/kedacore/http-add-on/tree/main/docs/contributing.md).\n\n---\nWe are a Cloud Native Computing Foundation (CNCF) incubation project.\n<p align="center"><img src="https://raw.githubusercontent.com/kedacore/keda/main/images/logo-cncf.svg" height="75px"></p>\n\n---\n\n## TL;DR\n\n```console\nhelm repo add kedacore https://kedacore.github.io/charts\nhelm repo update\n\nhelm install http-add-on kedacore/keda-add-ons-http --create-namespace --namespace keda\n```\n\n## Introduction\n\nThis chart bootstraps KEDA HTTP Add-on infrastructure on a Kubernetes cluster using the Helm package manager.\n\nAs part of that, it will install all the required Custom Resource Definitions (CRD).\n\n## Installing the Chart\n\nTo install the chart with the release name `http-add-on`, please read the [install instructions on the official repository to get started](https://github.com/kedacore/http-add-on/tree/main/docs/install.md):\n\n```console\n$ helm install http-add-on kedacore/keda-add-ons-http --namespace keda\n```\n\n> **Important:** This chart **needs** KEDA installed in your cluster to work properly.\n\n## Uninstalling the Chart\n\nTo uninstall/delete the `http-add-on` Helm chart:\n\n```console\nhelm uninstall http-add-on\n```\n\nThe command removes all the Kubernetes components associated with the chart and deletes the release.\n\n## Configuration\n\nThe following table lists the configurable parameters of the HTTP Add-On chart and\ntheir default values.\n\n### General parameters\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `additionalLabels` | string | `""` | Additional labels to be applied to installed resources. Note that not all resources will receive these labels. |\n| `crds.install` | bool | `true` | Whether to install the `HTTPScaledObject` [`CustomResourceDefinition`](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) |\n| `images.interceptor` | string | `"ghcr.io/kedacore/http-add-on-interceptor"` | Image name for the interceptor image component |\n| `images.kubeRbacProxy.name` | string | `"gcr.io/kubebuilder/kube-rbac-proxy"` | Image name for the Kube RBAC Proxy image component |\n| `images.kubeRbacProxy.tag` | string | `"v0.13.0"` | Image tag for the Kube RBAC Proxy image component |\n| `images.operator` | string | `"ghcr.io/kedacore/http-add-on-operator"` | Image name for the operator image component |\n| `images.scaler` | string | `"ghcr.io/kedacore/http-add-on-scaler"` | Image name for the scaler image component |\n| `images.tag` | string | `""` | Image tag for the http add on. This tag is applied to the images listed in `images.operator`, `images.interceptor`, and `images.scaler`. Optional, given app version of Helm chart is used by default |\n| `rbac.aggregateToDefaultRoles` | bool | `false` | Install aggregate roles for edit and view |\n\n### Operator\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `operator.adminPort` | int | `9090` | The port for the operator\'s admin server to run on |\n| `operator.adminService` | string | `"operator-admin"` | The name of the [`Service`](https://kubernetes.io/docs/concepts/services-networking/service/) for the operator\'s admin server |\n| `operator.affinity` | object | `{}` | Affinity for pod scheduling ([docs](https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/)) |\n| `operator.imagePullSecrets` | list | `[]` | The image pull secrets for the operator component |\n| `operator.nodeSelector` | object | `{}` | Node selector for pod scheduling ([docs](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)) |\n| `operator.port` | int | `8443` | The port for the operator main server to run on |\n| `operator.pullPolicy` | string | `"Always"` | The image pull policy for the operator component |\n| `operator.resources.limits` | object | `{"cpu":0.5,"memory":"64Mi"}` | The CPU/memory resource limit for the operator component |\n| `operator.resources.requests` | object | `{"cpu":"250m","memory":"20Mi"}` | The CPU/memory resource request for the operator component |\n| `operator.tolerations` | list | `[]` | Tolerations for pod scheduling ([docs](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)) |\n| `operator.watchNamespace` | string | `""` | The namespace to watch for new `HTTPScaledObject`s. Leave this blank (i.e. `""`) to tell the operator to watch all namespaces. |\n\n### Scaler\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `scaler.affinity` | object | `{}` | Affinity for pod scheduling ([docs](https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/)) |\n| `scaler.grpcPort` | int | `9090` | The port for the scaler\'s gRPC server. This is the server that KEDA will send scaling requests to. |\n| `scaler.healthPort` | int | `9091` | The port for the scaler\'s health check and admin server |\n| `scaler.imagePullSecrets` | list | `[]` | The image pull secrets for the scaler component |\n| `scaler.nodeSelector` | object | `{}` | Node selector for pod scheduling ([docs](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)) |\n| `scaler.pendingRequestsInterceptor` | int | `200` | The number of "target requests" that the external scaler will report to KEDA for the interceptor\'s scaling metrics. See the [KEDA external scaler documentation](https://keda.sh/docs/2.4/concepts/external-scalers/) for details on target requests. |\n| `scaler.pullPolicy` | string | `"Always"` | The image pull policy for the scaler component |\n| `scaler.resources.limits.cpu` | float | `0.5` |  |\n| `scaler.resources.limits.memory` | string | `"64Mi"` |  |\n| `scaler.resources.requests.cpu` | string | `"250m"` |  |\n| `scaler.resources.requests.memory` | string | `"20Mi"` |  |\n| `scaler.service` | string | `"external-scaler"` | The name of the Kubernetes `Service` for the scaler component |\n| `scaler.tolerations` | list | `[]` | Tolerations for pod scheduling ([docs](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)) |\n\n### Interceptor\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `interceptor.admin.port` | int | `9090` | The port for the interceptor\'s admin server to run on |\n| `interceptor.admin.service` | string | `"interceptor-admin"` | The name of the Kubernetes `Service` for the interceptor\'s admin service |\n| `interceptor.affinity` | object | `{}` | Affinity for pod scheduling ([docs](https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/)) |\n| `interceptor.deploymentCachePollingIntervalMS` | int | `250` | How often (in milliseconds) the interceptor does a full refresh of its deployment cache. The interceptor will also use Kubernetes events to stay up-to-date with the deployment cache changes. This duration is the maximum time it will take to see changes to the deployment state. |\n| `interceptor.expectContinueTimeout` | string | `"1s"` | Special handling for responses with "Expect: 100-continue" response headers. see https://pkg.go.dev/net/http#Transport under the \'ExpectContinueTimeout\' field for more details |\n| `interceptor.forceHTTP2` | bool | `false` | Whether or not the interceptor should force requests to use HTTP/2 |\n| `interceptor.idleConnTimeout` | string | `"90s"` | The timeout after which any idle connection is closed and removed from the interceptor\'s in-memory connection pool. |\n| `interceptor.imagePullSecrets` | list | `[]` | The image pull secrets for the interceptor component |\n| `interceptor.keepAlive` | string | `"1s"` | The interceptor\'s connection keep alive timeout |\n| `interceptor.maxIdleConns` | int | `100` | The maximum number of idle connections allowed in the interceptor\'s in-memory connection pool. Set to 0 to indicate no limit |\n| `interceptor.nodeSelector` | object | `{}` | Node selector for pod scheduling ([docs](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)) |\n| `interceptor.proxy.port` | int | `8080` | The port on which the interceptor\'s proxy service will listen for live HTTP traffic |\n| `interceptor.proxy.service` | string | `"interceptor-proxy"` | The name of the Kubernetes `Service` for the interceptor\'s proxy service. This is the service that accepts live HTTP traffic. |\n| `interceptor.pullPolicy` | string | `"Always"` | The image pull policy for the interceptor component |\n| `interceptor.replicas.max` | int | `50` | The maximum number of interceptor replicas that should ever be running |\n| `interceptor.replicas.min` | int | `3` | The minimum number of interceptor replicas that should ever be running |\n| `interceptor.replicas.waitTimeout` | string | `"20s"` | The maximum time the interceptor should wait for an HTTP request to reach a backend before it is considered a failure |\n| `interceptor.resources.limits` | object | `{"cpu":0.5,"memory":"64Mi"}` | The CPU/memory resource limit for the operator component |\n| `interceptor.resources.requests` | object | `{"cpu":"250m","memory":"20Mi"}` | The CPU/memory resource request for the operator component |\n| `interceptor.responseHeaderTimeout` | string | `"500ms"` | How long the interceptor will wait between forwarding a request to a backend and receiving response headers back before failing the request |\n| `interceptor.scaledObject.pollingInterval` | int | `1` | The interval (in milliseconds) that KEDA should poll the external scaler to fetch scaling metrics about the interceptor |\n| `interceptor.tcpConnectTimeout` | string | `"500ms"` | How long the interceptor waits to establish TCP connections with backends before failing a request. |\n| `interceptor.tlsHandshakeTimeout` | string | `"10s"` | The maximum amount of time the interceptor will wait for a TLS handshake. Set to zero to indicate no timeout. |\n| `interceptor.tolerations` | list | `[]` | Tolerations for pod scheduling ([docs](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)) |\n\nSpecify each parameter using the `--set key=value[,key=value]` argument to\n`helm install`. For example:\n\n```console\n$ helm install http-add-on kedacore/keda-add-ons-http --namespace keda \\\n               --set version=<different tag from app version>\n```\n\nAlternatively, a YAML file that specifies the values for the above parameters can\nbe provided while installing the chart. For example,\n\n```console\nhelm install http-add-on kedacore/keda-add-ons-http --namespace keda -f values.yaml\n```\n\n----------------------------------------------\nAutogenerated from chart metadata using [helm-docs](https://github.com/norwoodj/helm-docs)\n'
      if name == 'datree-admission-webhook':
        return '# Datree Admission Webhook\n\nA Kubernetes validating webhook for policy enforcement within the cluster, on every CREATE, APPLY and UPDATE operation\non a resource.\n\n## TL;DR\n\n```bash\n  # Install and create namespace with Helm\n  helm repo add datree-webhook https://datreeio.github.io/admission-webhook-datree/\n  helm repo update\n\n  # Already existing `datree` namespace\n  kubectl create ns datree\n  helm install -n datree datree-webhook datree-webhook/datree-admission-webhook --set datree.token=<DATREE_TOKEN>\n```\n\n### Prerequisites\n\nHelm v3.0.0+\n\n## Configuration Options\n\nDatree admission webhook can be configured via the helm values file under `datree` key:\n\n### Datree Configuration options\n\n```\ndatree:\n  token: <DATREE_TOKEN>     # The token used to link the CLI to your dashboard.\n  policy: ""                # The name of the policy to check, e.g: staging. (string, optional)\n  verbose: ""               # Display \'How to Fix\' link for failed rules in output. (boolean ,optional)\n  output: ""                # The format output of the policy check results: yaml, json, xml, simple, JUnit. (string ,optional)\n  noRecord: ""              # Don’t send policy checks metadata to the backend. (boolean ,optional)\n  enforce: ""               # Block resources that fail the policy check. (boolean ,optional)\n  clusterName: ""           # The name of the cluster link for cluster name in your dashboard. (string ,optional)\n```\n\nFor further information about Datree flags see [CLI arguments](https://hub.datree.io/setup/cli-arguments).\n\n### Parameters\n\n## Values\n\n<table>\n\t<thead>\n\t\t<th>Parameter</th>\n\t\t<th>Description</th>\n\t\t<th>Default</th>\n\t</thead>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td>namespace</td>\n\t\t\t<td>The name of the namespace all resources will be created in, if not specified in the release.</td>\n\t\t\t<td><pre lang="json">\n""\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>replicaCount</td>\n\t\t\t<td>The number of Datree webhook-server replicas to deploy for the webhook.</td>\n\t\t\t<td><pre lang="json">\n2\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>customLabels</td>\n\t\t\t<td>Additional labels to add to all resources.</td>\n\t\t\t<td><pre lang="json">\n{}\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>customAnnotations</td>\n\t\t\t<td>Additional annotations to add to all resources.</td>\n\t\t\t<td><pre lang="json">\n{}\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>rbac.serviceAccount</td>\n\t\t\t<td>Create service Account for the webhook</td>\n\t\t\t<td><pre lang="json">\n{\n  "create": true,\n  "name": "datree-webhook-server"\n}\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>rbac.clusterRole</td>\n\t\t\t<td>Create service Role for the webhook</td>\n\t\t\t<td><pre lang="json">\n{\n  "create": true,\n  "name": "datree-webhook-server-cluster-role"\n}\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>datree.token</td>\n\t\t\t<td>The token used to link Datree to your dashboard. (string, required)</td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>datree.existingSecret</td>\n\t\t\t<td>The token may also be provided via secret, note if the existingSecret is provided the token field above is ignored.</td>\n\t\t\t<td><pre lang="json">\n{\n  "key": "",\n  "name": ""\n}\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>datree.verbose</td>\n\t\t\t<td>Display \'How to Fix\' link for failed rules in output. (boolean, optional)</td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>datree.output</td>\n\t\t\t<td>The format output of the policy check results: yaml, json, xml, simple, JUnit. (string, optional)</td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>datree.noRecord</td>\n\t\t\t<td>Don’t send policy checks metadata to the backend. (boolean, optional)</td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>datree.clusterName</td>\n\t\t\t<td>The name of the cluster link for cluster name in your dashboard (string ,optional)</td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>datree.scanIntervalHours</td>\n\t\t\t<td>How often should the scan run in hours. (int, optional, default: 1 )</td>\n\t\t\t<td><pre lang="json">\n1\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>datree.configFromHelm</td>\n\t\t\t<td>If false, the webhook will be configured from the dashboard, otherwise it will be configured from here. Affected configurations: policy, enforce, customSkipList.</td>\n\t\t\t<td><pre lang="json">\nfalse\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>datree.policy</td>\n\t\t\t<td>The name of the policy to check, e.g: staging. (string, optional)</td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>datree.enforce</td>\n\t\t\t<td>Block resources that fail the policy check. (boolean ,optional)</td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>datree.customSkipList</td>\n\t\t\t<td>Excluded resources from policy checks. ("namespace;kind;name" ,optional)</td>\n\t\t\t<td><pre lang="json">\n[\n  "(.*);(.*);(^aws-node.*)"\n]\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>image.repository</td>\n\t\t\t<td>Image repository for the webhook</td>\n\t\t\t<td><pre lang="json">\n"datree/admission-webhook"\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>image.tag</td>\n\t\t\t<td>The image release tag to use for the webhook</td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>image.pullPolicy</td>\n\t\t\t<td>Image pull policy for the webhook</td>\n\t\t\t<td><pre lang="json">\n"Always"\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>imageCredentials</td>\n\t\t\t<td>For private registry which contains all the required images</td>\n\t\t\t<td><pre lang="json">\n{\n  "email": null,\n  "enabled": false,\n  "password": null,\n  "registry": null,\n  "username": null\n}\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>securityContext</td>\n\t\t\t<td>Security context applied on the containers</td>\n\t\t\t<td><pre lang="json">\n{\n  "allowPrivilegeEscalation": false,\n  "capabilities": {\n    "drop": [\n      "ALL"\n    ]\n  },\n  "readOnlyRootFilesystem": true,\n  "runAsNonRoot": true,\n  "runAsUser": 25000,\n  "seccompProfile": {\n    "type": "RuntimeDefault"\n  }\n}\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>resources</td>\n\t\t\t<td>The resource request/limits for the webhook container image</td>\n\t\t\t<td><pre lang="json">\n{}\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>nodeSelector</td>\n\t\t\t<td>Used to select on which node a pod is scheduled to run</td>\n\t\t\t<td><pre lang="json">\n{}\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>affinity</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\n{}\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>tolerations</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\n[]\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>clusterScanner.resources</td>\n\t\t\t<td>The resource request/limits for the scanner container image</td>\n\t\t\t<td><pre lang="json">\n{}\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>clusterScanner.annotations</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\n{}\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>clusterScanner.rbac.serviceAccount</td>\n\t\t\t<td>Create service Account for the scanner</td>\n\t\t\t<td><pre lang="json">\n{\n  "create": true,\n  "name": "cluster-scanner-service-account"\n}\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>clusterScanner.rbac.clusterRole</td>\n\t\t\t<td>Create service Role for the scanner</td>\n\t\t\t<td><pre lang="json">\n{\n  "create": true,\n  "name": "cluster-scanner-role"\n}\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>clusterScanner.rbac.clusterRoleBinding</td>\n\t\t\t<td>Create service RoleBinding for the scanner</td>\n\t\t\t<td><pre lang="json">\n{\n  "name": "cluster-scanner-role-binding"\n}\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>clusterScanner.image.repository</td>\n\t\t\t<td>Image repository for the scanner</td>\n\t\t\t<td><pre lang="json">\n"datree/cluster-scanner"\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>clusterScanner.image.pullPolicy</td>\n\t\t\t<td>Image pull policy for the scanner</td>\n\t\t\t<td><pre lang="json">\n"Always"\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>clusterScanner.image.tag</td>\n\t\t\t<td>The image release tag to use for the scanner</td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>clusterScanner.image.resources</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\n{}\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>clusterScanner.livenessProbe.enabled</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\ntrue\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>clusterScanner.livenessProbe.scheme</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>clusterScanner.livenessProbe.initialDelaySeconds</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>clusterScanner.livenessProbe.periodSeconds</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>clusterScanner.readinessProbe.enabled</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\ntrue\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>clusterScanner.readinessProbe.scheme</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>clusterScanner.readinessProbe.initialDelaySeconds</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>clusterScanner.readinessProbe.periodSeconds</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>hooks.timeoutTime</td>\n\t\t\t<td>The timeout time the hook will wait for the webhook-server is ready.</td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>hooks.ttlSecondsAfterFinished</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>hooks.image.repository</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\n"clastix/kubectl"\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>hooks.image.tag</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\n"v1.25"\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>hooks.image.pullPolicy</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\n"IfNotPresent"\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>validatingWebhookConfiguration.failurePolicy</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\n"Ignore"\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>livenessProbe.enabled</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\ntrue\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>livenessProbe.scheme</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>livenessProbe.initialDelaySeconds</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>livenessProbe.periodSeconds</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>readinessProbe.enabled</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\ntrue\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>readinessProbe.scheme</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>readinessProbe.initialDelaySeconds</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>readinessProbe.periodSeconds</td>\n\t\t\t<td></td>\n\t\t\t<td><pre lang="json">\nnull\n</pre>\n</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n'
      if name == 'external-scaler-azure-cosmos-db':
        return '# KEDA External Scaler for Azure Cosmos DB\r\n\r\nChart for installing KEDA external scaler for Azure Cosmos DB.\r\n\r\n- [Documentation](https://github.com/kedacore/external-scaler-azure-cosmos-db#readme)\r\n- [Release Notes](https://github.com/kedacore/external-scaler-azure-cosmos-db/releases/tag/v0.1.0)\r\n- [Example Usage](https://github.com/kedacore/external-scaler-azure-cosmos-db/tree/main/src/Scaler.Demo)\r\n\r\n## Installation\r\n\r\n1. Add and update Helm chart repo.\r\n\r\n    ```shell\r\n    helm repo add kedacore https://kedacore.github.io/charts\r\n    helm repo update\r\n    ```\r\n\r\n1. Install KEDA Helm chart (*or follow one of the other installation methods on [KEDA documentation](https://keda.sh/docs/deploy)*).\r\n\r\n    ```shell\r\n    helm install keda kedacore/keda --namespace keda --create-namespace\r\n    ```\r\n\r\n1. Install Azure Cosmos DB external scaler Helm chart.\r\n\r\n    ```shell\r\n    helm install external-scaler-azure-cosmos-db kedacore/external-scaler-azure-cosmos-db --namespace keda --create-namespace\r\n    ```\r\n\r\n## Values\r\n\r\n| Key | Type | Default | Description |\r\n|---|---|---|---|\r\n| additionalLabels | object | `{}` | Additional labels that should be applied to all resources |\r\n| image.pullPolicy | string | `"Always"` | The image pull policy for Azure Cosmos DB external scaler |\r\n| image.repository | string | `"ghcr.io/kedacore/external-scaler-azure-cosmos-db"` | The Docker image repository to use for Azure Cosmos DB external scaler |\r\n| image.tag | string | `"0.1.0"` | The Docker image tag to use for Azure Cosmos DB external scaler |\r\n| port | int | `4050` | The incoming port for \'Azure Cosmos DB external scaler\' service |\r\n| resources.limits.cpu | string | `"100m"` | Maximum limit on CPU for \'Azure Cosmos DB external scaler\' pod |\r\n| resources.limits.memory | string | `"512Mi"` | Maximum limit on memory for \'Azure Cosmos DB external scaler\' pod |\r\n| resources.requests.cpu | string | `"10m"` | Initial CPU request by \'Azure Cosmos DB external scaler\' pod |\r\n| resources.requests.memory | string | `"128Mi"` | Initial memory request by \'Azure Cosmos DB external scaler\' pod |\r\n'
      return ''

